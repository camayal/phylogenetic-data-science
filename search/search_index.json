{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Phylogenetic Data Science \u00b6 Welcome to Phylogenetic Data Science , a draft on-line textbook for learning computational phylogenetics through interactive coding exercises in Python. A goal of this textbook is to introduce a new generation of students to algorithms used in phylogenetics and population genetics for studying evolutionary relationships from phylogenetic trees, and to introduce these concepts through a data-driven approach geared towards treating phylogenetics as a form of data science. Ethos \u00b6 free: all resources are available free and on-line. easy: coding exercises can be run on cloud-based servers using any computer. interactive: learn by doing; equations and algorithms are paired with example code. community: anyone can contribute changes for editorial approval. translatable: a Python bootcamp and coding exercises aim to demonstrate modern best practices. modern: chapters reference recent advances and software tools. organized: on-line search enables fast lookup of examples and references. Developers/Contributors \u00b6 Deren Eaton , Columbia University Carlos Alonso Maya-Lastra , Columbia University Contributing \u00b6 This textbook is completely free and open source, and is intended to be a living document that will evolve and improve over time. Anyone can contribute to either the text or code, and we strongly encourage everyone to consider contributing if you find any errors, or have ideas for improvements. You can easily contribute to this textbook by clicking the pencil icon ( ) in the upper right corner of any page. This will open a Markdown document of the page which you can fork, edit, and contribute back through a pull request. Funding \u00b6 The development of this textbook was supported by NSF grant DEB-2046813 to D. Eaton. License \u00b6 All text and code encompassing this textbook is hosted on GitHub under the GPLv3 licence .","title":"About/Ethos"},{"location":"#phylogenetic-data-science","text":"Welcome to Phylogenetic Data Science , a draft on-line textbook for learning computational phylogenetics through interactive coding exercises in Python. A goal of this textbook is to introduce a new generation of students to algorithms used in phylogenetics and population genetics for studying evolutionary relationships from phylogenetic trees, and to introduce these concepts through a data-driven approach geared towards treating phylogenetics as a form of data science.","title":"Phylogenetic Data Science"},{"location":"#ethos","text":"free: all resources are available free and on-line. easy: coding exercises can be run on cloud-based servers using any computer. interactive: learn by doing; equations and algorithms are paired with example code. community: anyone can contribute changes for editorial approval. translatable: a Python bootcamp and coding exercises aim to demonstrate modern best practices. modern: chapters reference recent advances and software tools. organized: on-line search enables fast lookup of examples and references.","title":"Ethos"},{"location":"#developerscontributors","text":"Deren Eaton , Columbia University Carlos Alonso Maya-Lastra , Columbia University","title":"Developers/Contributors"},{"location":"#contributing","text":"This textbook is completely free and open source, and is intended to be a living document that will evolve and improve over time. Anyone can contribute to either the text or code, and we strongly encourage everyone to consider contributing if you find any errors, or have ideas for improvements. You can easily contribute to this textbook by clicking the pencil icon ( ) in the upper right corner of any page. This will open a Markdown document of the page which you can fork, edit, and contribute back through a pull request.","title":"Contributing"},{"location":"#funding","text":"The development of this textbook was supported by NSF grant DEB-2046813 to D. Eaton.","title":"Funding"},{"location":"#license","text":"All text and code encompassing this textbook is hosted on GitHub under the GPLv3 licence .","title":"License"},{"location":"course/","text":"Computational phylogenetics EEEB G6500 \u00b6 During Spring 2022 a computational phylogenetics graduate level course will be assisting in the development of this textbook through reading discussions and coding exercises. Please refer to the syllabus .","title":"Columbia course"},{"location":"course/#computational-phylogenetics-eeeb-g6500","text":"During Spring 2022 a computational phylogenetics graduate level course will be assisting in the development of this textbook through reading discussions and coding exercises. Please refer to the syllabus .","title":"Computational phylogenetics EEEB G6500"},{"location":"installation/","text":"Installation and Software \u00b6 You can complete all exercises in this textbook without installing anything. That's right, all you really need is a web-browser. The exercises are hosted on a free cloud-based service called MyBinder , which allows you to access Jupyter notebooks running on a remote server with all required software pre-installed. You can find links to these notebooks in each chapter, and all links are also listed at the end of each chapter in the table of contents. As an example, the following link connects you to a notebook from the Coding bootcamp on learning to use Jupyter: ( jupyter-exercise.ipynb) . Nevertheless, you may wish to install the software dependencies and run code from the textbook exercises locally on your computer, which can provide a much greater opportunity for you to explore, modify, and learn about the code. Below I provide a brief summary of what this involves, and a link to our installation instructions. Software used in this textbook \u00b6 This textbook aims to teach basic and advanced Python coding skills alongside lessons on phylogenetics with the goal of making complex algorithms more easily understandable as a set of simpler step-by-step routines. This is a relatively difficult task, and obviously we cannot cover all aspects of Python coding, phylogenetic theory, and phylogenetic software in this book. In general, decisions about which subjects to focus on, how to write code, and which existing tools to highlight, uses the following guidelines: Coding style follows the 'Zen of Python' , which includes such enlightened ideas as \" Beautiful is better than ugly \", \"Simple is better than complex \", and \" Readability counts \". All software and package versions are clearly shown and updated frequently. All demonstrated software is easy to install (specifically, using conda ). ... Python libraries \u00b6 Whenever possible we strive to use as few Python dependencies as possible, and to develop examples using the most common libraries for scientific programing in Python. Currently this includes the following: Python Data Science: Numpy : arrays, tensors, and numeric processing. Pandas : tabular dataframes, statistical processing. Scipy : statistical distributions. Numba : just-in-time compilation for speed-optimized functions. Visualization: Toyplot : minimalist interactive vector-based plotting. Tree objects: Toytree : tree-based operations and tree plotting using toyplot. Coalescent simulations: Ipcoal : coalescent simulation library integrated with toytree visualization. Msprime : coalescent simulation backend for ipcoal. Tskit : tree sequence class backend for msprime. Machine learning: scikit-learn : simple and well documented machine learning code. External binaries \u00b6 In addition to learning to implement phylogenetic algorithms in Python, we also demonstrate usage of established external software tools in many exercises. When multiple options are available we tend to select tools that are easiest to install (e.g., using conda), and which have a command line interface (as opposed to graphical user interface). This includes the following software tools: Compiled software tools: RAxML : Maximum likelihood tree inference software. MrBayes : Bayesian tree inference software. BPP : Multispecies coalescent species tree software. ... Optional Installation Instructions \u00b6 If you want to follow along with the exercises in this textbook on your own computer, rather than use the cloud-based binder notebooks, you can do so by installing the necessary software into a conda environment. You can find detailed instructions for this in the Coding bootcamp .","title":"Installation/Software"},{"location":"installation/#installation-and-software","text":"You can complete all exercises in this textbook without installing anything. That's right, all you really need is a web-browser. The exercises are hosted on a free cloud-based service called MyBinder , which allows you to access Jupyter notebooks running on a remote server with all required software pre-installed. You can find links to these notebooks in each chapter, and all links are also listed at the end of each chapter in the table of contents. As an example, the following link connects you to a notebook from the Coding bootcamp on learning to use Jupyter: ( jupyter-exercise.ipynb) . Nevertheless, you may wish to install the software dependencies and run code from the textbook exercises locally on your computer, which can provide a much greater opportunity for you to explore, modify, and learn about the code. Below I provide a brief summary of what this involves, and a link to our installation instructions.","title":"Installation and Software"},{"location":"installation/#software-used-in-this-textbook","text":"This textbook aims to teach basic and advanced Python coding skills alongside lessons on phylogenetics with the goal of making complex algorithms more easily understandable as a set of simpler step-by-step routines. This is a relatively difficult task, and obviously we cannot cover all aspects of Python coding, phylogenetic theory, and phylogenetic software in this book. In general, decisions about which subjects to focus on, how to write code, and which existing tools to highlight, uses the following guidelines: Coding style follows the 'Zen of Python' , which includes such enlightened ideas as \" Beautiful is better than ugly \", \"Simple is better than complex \", and \" Readability counts \". All software and package versions are clearly shown and updated frequently. All demonstrated software is easy to install (specifically, using conda ). ...","title":"Software used in this textbook"},{"location":"installation/#python-libraries","text":"Whenever possible we strive to use as few Python dependencies as possible, and to develop examples using the most common libraries for scientific programing in Python. Currently this includes the following: Python Data Science: Numpy : arrays, tensors, and numeric processing. Pandas : tabular dataframes, statistical processing. Scipy : statistical distributions. Numba : just-in-time compilation for speed-optimized functions. Visualization: Toyplot : minimalist interactive vector-based plotting. Tree objects: Toytree : tree-based operations and tree plotting using toyplot. Coalescent simulations: Ipcoal : coalescent simulation library integrated with toytree visualization. Msprime : coalescent simulation backend for ipcoal. Tskit : tree sequence class backend for msprime. Machine learning: scikit-learn : simple and well documented machine learning code.","title":"Python libraries"},{"location":"installation/#external-binaries","text":"In addition to learning to implement phylogenetic algorithms in Python, we also demonstrate usage of established external software tools in many exercises. When multiple options are available we tend to select tools that are easiest to install (e.g., using conda), and which have a command line interface (as opposed to graphical user interface). This includes the following software tools: Compiled software tools: RAxML : Maximum likelihood tree inference software. MrBayes : Bayesian tree inference software. BPP : Multispecies coalescent species tree software. ...","title":"External binaries"},{"location":"installation/#optional-installation-instructions","text":"If you want to follow along with the exercises in this textbook on your own computer, rather than use the cloud-based binder notebooks, you can do so by installing the necessary software into a conda environment. You can find detailed instructions for this in the Coding bootcamp .","title":"Optional Installation Instructions"},{"location":"bootcamp/0.0-installation-conda/","text":"Installing software with conda \u00b6 What is conda? \u00b6 Conda is an open-source software management tool for installing other software, as well as their dependencies, and creating sandboxed environments for executing code. Using the conda command line tool you can use simple commands to search for software packages, select specific versions, and install them locally on your machine. This automated process makes installing and removing software simple and reproducible which makes it easier to design, distribute and use working software. Why use conda? \u00b6 The many advantages of using conda include: command-line convenience : the conda command line program allows you to search for and install tools with simple commands that can even be written as scripts for automation. This makes it easy to replicate the set of software tools installed on one computer onto another machine. finding dependencies : Almost every software program builds on and requires other software packages as dependencies. Rather than telling a user to go find and install each of these dependencies on their own (a sure sign of a poorly developed tool by today's standards) a software package manager can instead fetch and install of the dependencies for them. This might even include different dependencies or versions depending on their specific operating system. This is a very complex task and something conda does very well. sandboxed directory : conda installs software into a sandboxed location on your computer (usually a directory within $HOME ), which is done purposefully to keep your conda software completely separate and isolated from your system-wide software (which is usually in /bin or /usr/bin ). This gives you peace of mind to install, update, and remove packages as much as you want inside of your conda directory without having to worry that it might ever impact your system programs. environments : In addition to allowing you to install software programs into a sandboxed location, conda also allows you to keep many separate environments , where you can keep different sets of software or versions of them. This makes it easy to test software tools across different version of dependencies, or to keep software separate that uses different conflicting dependencies. Install conda (miniconda3) \u00b6 There are two flavors of conda that you can install: Anaconda and Miniconda . Both include an installation of Python and the conda program (which is written in Python) as well as a few dependencies of conda . However, the two flavors differ in terms of which other tools come pre-loaded with these basics. Anaconda comes fully loaded with dozens of commonly used Python packages, whereas Miniconda is totally minimal, and doesn't come with anything extra at all. I always recommend installing Miniconda , and later adding to it any software that you want to install. To install Miniconda follow the command-line instructions below, which are derived from the Miniconda docs . When installing either flavor of conda it is important that you select the appropriate version for your operating system (e.g., MacOSX, Linux, Windows). Attention If you are on Windows I generally recommend installing Windows Subsystem for Linux (WSL2) , which allows you to run a fully functional Linux system inside of your Windows machine. From there, you can then run all of the Linux-based installation instructions. In general this is much more reliable for running scientific software, since many tools are not written for Windows. If you do not yet have conda installed then copy and paste the following code into your terminal to download and install conda into its default (recommended) location in a folder in your $HOME directory that will be named miniconda3/ . Linux OSX # download installer script wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh # call the installer script in batch (-b) mode bash Miniconda3-latest-Linux-x86_64.sh -b # call the init script (see below for description of what this does.) ~/miniconda3/condabin/conda init # download installer script (flag is a big letter o, not zero) curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-MacOSX-x86_64.sh # call the installer script in batch (-b) mode bash Miniconda3-latest-MacOSX-x86_64.sh -b # call the init script (see below for description of what this does.) ~/miniconda3/condabin/conda init To validate that you have conda properly installed you can run the following commands from your terminal: # prints the path to your conda binary which conda # prints information about your conda installation conda info # prints the software currently installed in your conda directory conda list Conda and $PATH \u00b6 After running the commands above you will likely see a small change to the text that is shown before the cursor in your terminal. Perhaps it shows something like (base) . This is telling you the name of the conda environment that you currently have loaded. An environment is simply a subfolder within your miniconda3/ directory where software is stored. You can create multiple environments with different sets of software for different projects. If you just installed conda then your base environment will hardly have any software in it. The purpose of having an environment loaded is to make software from that particular folder accessible. This relates to the way that you shell looks for software, which is by looking in a set of specified folders in order until it either finds the first occurrence of the requested software name, or no occurrence of it. This set of ordered folders is called your PATH variable, and you can view it from the command shell. Below I show an example in a Linux shell: # show your current shell PATH variable echo $PATH # /home/deren/miniconda3/bin:/home/deren/miniconda3/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin This shows multiple paths to directories each separated by a colon. You should notice that there are two references to miniconda3 in the PATH . One points to .../miniconda3/condabin/... and another to .../miniconda3/bin . The former is the location where your conda binary is located -- the tool that is used to call conda commands including loading or deactivating environments. The second is a path to binaries (programs) in the base conda environment. This includes the version of Python that is installed. Towards the end of the PATH you will find references to other directories such as /usr/bin . These are places where system-wide software is located. Python is also used by most operating systems to complete mundane tasks, but that system-wide version is different from the one we just installed -- they are installed in different places. So, when we now type python (the name of the program to start a Python interpreter) into the terminal it needs to decide which of the multiple versions of python present on the system it should execute. Because the binary in our miniconda3 directory is located before the path to the system-wide binary in our PATH variable the miniconda version will be loaded. We can verify this quickly by deactivating the conda environment, and then re-loading it. This is something worth becoming comfortable with. You want to know which conda environment is loaded at any time so you know which software versions you are working with ( this is why the env name is shown is the prompt of your terminal ). # deactivate the current conda env conda deactivate # print the PATH now (`miniconda/bin` should be gone) echo $PATH # re-activate the conda environment conda activate base # print the PATH again (`miniconda/bin` should be back) echo $PATH Now that you have conda installed you can use it to install software, create and load environments, and/or remove software and environments. See the following conda cheatsheet for some tips on available commands. Installing packages (using channels) \u00b6 Many many many software packages are available to install using conda . The best way to find instructions to install a tool is to google search 'conda install tool-name' . (You can also run the command conda search toolname from a terminal, but I find the former is generally faster). This will provide the proper instructions for installing the package, including the name of a channel from which it should be installed. Here is an example that will install the requests Python package from the conda-forge channel. # install the requests Python package from the conda-forge channel conda install requests -c conda-forge Tip The channel conda-forge is special in that it contains very up-to-date versions of most common Python packages. In this way, it serves as an alternative to the standard channel (listing no channel at all). If you use conda-forge for some packages, it is generally recommended to use it for all of your packages, otherwise, you will continually have versions of standard Python dependencies being switched back and forth between super up-to-date versions from conda-forge and not quite as up-to-date versions from the standard channel. The easiest way to ensure that you don't forget to do this is to set conda-forge as your default channel , which you can do with the following command: conda config --add channels conda-forge Creating an environment \u00b6 As mentioned already, you can create separate environments and install separate versions of software into them. As an example, let's create a new environment called 'test', install a package into it, and then remove it, since it is just a test example. This example also shows another important lesson: you can list multiple channels which will be searched in sorted order (like PATH) to find the package and its dependencies. Here I install raxml which will look first in the conda-forge and then bioconda channels. # create new env, activate, install softare, and call software conda create -n test conda activate test conda install raxml -c conda-forge -c bioconda raxmlHPC -v # switch back to the 'base' env and remove 'test' env conda activate base conda env remove -n test An environment for this textbook \u00b6 Finally, we've reached the point where you are ready to install all of the software associated with exercises in this textbook. As you might expect, I recommend installing this into a conda environment. This can be done easily using a special file called environment.yml , which is used to provide instructions to conda for installing many packages. example environment.yml file name : phylodatascience channels : - conda-forge - bioconda dependencies : - python>=3.7 - jupyter - numpy - pandas - scipy - numba - loguru - toytree>=3.0 - msprime>=1.0 - raxml # ...TODO You can get this file by cloning the git repository for this textbook. # clone the repo to a location (I use ~/Documents/) and cd into it. cd ~/Documents/ git clone https://github.com/eaton-lab/phylogenetic-data-science cd phylogenetic-data-science/ # create new env and install all dependencies into it. conda env create -f environment.yml # activate this environment conda activate phylodatascience","title":"0.0 - Installation (conda)"},{"location":"bootcamp/0.0-installation-conda/#installing-software-with-conda","text":"","title":"Installing software with conda"},{"location":"bootcamp/0.0-installation-conda/#what-is-conda","text":"Conda is an open-source software management tool for installing other software, as well as their dependencies, and creating sandboxed environments for executing code. Using the conda command line tool you can use simple commands to search for software packages, select specific versions, and install them locally on your machine. This automated process makes installing and removing software simple and reproducible which makes it easier to design, distribute and use working software.","title":"What is conda?"},{"location":"bootcamp/0.0-installation-conda/#why-use-conda","text":"The many advantages of using conda include: command-line convenience : the conda command line program allows you to search for and install tools with simple commands that can even be written as scripts for automation. This makes it easy to replicate the set of software tools installed on one computer onto another machine. finding dependencies : Almost every software program builds on and requires other software packages as dependencies. Rather than telling a user to go find and install each of these dependencies on their own (a sure sign of a poorly developed tool by today's standards) a software package manager can instead fetch and install of the dependencies for them. This might even include different dependencies or versions depending on their specific operating system. This is a very complex task and something conda does very well. sandboxed directory : conda installs software into a sandboxed location on your computer (usually a directory within $HOME ), which is done purposefully to keep your conda software completely separate and isolated from your system-wide software (which is usually in /bin or /usr/bin ). This gives you peace of mind to install, update, and remove packages as much as you want inside of your conda directory without having to worry that it might ever impact your system programs. environments : In addition to allowing you to install software programs into a sandboxed location, conda also allows you to keep many separate environments , where you can keep different sets of software or versions of them. This makes it easy to test software tools across different version of dependencies, or to keep software separate that uses different conflicting dependencies.","title":"Why use conda?"},{"location":"bootcamp/0.0-installation-conda/#install-conda-miniconda3","text":"There are two flavors of conda that you can install: Anaconda and Miniconda . Both include an installation of Python and the conda program (which is written in Python) as well as a few dependencies of conda . However, the two flavors differ in terms of which other tools come pre-loaded with these basics. Anaconda comes fully loaded with dozens of commonly used Python packages, whereas Miniconda is totally minimal, and doesn't come with anything extra at all. I always recommend installing Miniconda , and later adding to it any software that you want to install. To install Miniconda follow the command-line instructions below, which are derived from the Miniconda docs . When installing either flavor of conda it is important that you select the appropriate version for your operating system (e.g., MacOSX, Linux, Windows). Attention If you are on Windows I generally recommend installing Windows Subsystem for Linux (WSL2) , which allows you to run a fully functional Linux system inside of your Windows machine. From there, you can then run all of the Linux-based installation instructions. In general this is much more reliable for running scientific software, since many tools are not written for Windows. If you do not yet have conda installed then copy and paste the following code into your terminal to download and install conda into its default (recommended) location in a folder in your $HOME directory that will be named miniconda3/ . Linux OSX # download installer script wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh # call the installer script in batch (-b) mode bash Miniconda3-latest-Linux-x86_64.sh -b # call the init script (see below for description of what this does.) ~/miniconda3/condabin/conda init # download installer script (flag is a big letter o, not zero) curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-MacOSX-x86_64.sh # call the installer script in batch (-b) mode bash Miniconda3-latest-MacOSX-x86_64.sh -b # call the init script (see below for description of what this does.) ~/miniconda3/condabin/conda init To validate that you have conda properly installed you can run the following commands from your terminal: # prints the path to your conda binary which conda # prints information about your conda installation conda info # prints the software currently installed in your conda directory conda list","title":"Install conda (miniconda3)"},{"location":"bootcamp/0.0-installation-conda/#conda-and-path","text":"After running the commands above you will likely see a small change to the text that is shown before the cursor in your terminal. Perhaps it shows something like (base) . This is telling you the name of the conda environment that you currently have loaded. An environment is simply a subfolder within your miniconda3/ directory where software is stored. You can create multiple environments with different sets of software for different projects. If you just installed conda then your base environment will hardly have any software in it. The purpose of having an environment loaded is to make software from that particular folder accessible. This relates to the way that you shell looks for software, which is by looking in a set of specified folders in order until it either finds the first occurrence of the requested software name, or no occurrence of it. This set of ordered folders is called your PATH variable, and you can view it from the command shell. Below I show an example in a Linux shell: # show your current shell PATH variable echo $PATH # /home/deren/miniconda3/bin:/home/deren/miniconda3/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin This shows multiple paths to directories each separated by a colon. You should notice that there are two references to miniconda3 in the PATH . One points to .../miniconda3/condabin/... and another to .../miniconda3/bin . The former is the location where your conda binary is located -- the tool that is used to call conda commands including loading or deactivating environments. The second is a path to binaries (programs) in the base conda environment. This includes the version of Python that is installed. Towards the end of the PATH you will find references to other directories such as /usr/bin . These are places where system-wide software is located. Python is also used by most operating systems to complete mundane tasks, but that system-wide version is different from the one we just installed -- they are installed in different places. So, when we now type python (the name of the program to start a Python interpreter) into the terminal it needs to decide which of the multiple versions of python present on the system it should execute. Because the binary in our miniconda3 directory is located before the path to the system-wide binary in our PATH variable the miniconda version will be loaded. We can verify this quickly by deactivating the conda environment, and then re-loading it. This is something worth becoming comfortable with. You want to know which conda environment is loaded at any time so you know which software versions you are working with ( this is why the env name is shown is the prompt of your terminal ). # deactivate the current conda env conda deactivate # print the PATH now (`miniconda/bin` should be gone) echo $PATH # re-activate the conda environment conda activate base # print the PATH again (`miniconda/bin` should be back) echo $PATH Now that you have conda installed you can use it to install software, create and load environments, and/or remove software and environments. See the following conda cheatsheet for some tips on available commands.","title":"Conda and $PATH"},{"location":"bootcamp/0.0-installation-conda/#installing-packages-using-channels","text":"Many many many software packages are available to install using conda . The best way to find instructions to install a tool is to google search 'conda install tool-name' . (You can also run the command conda search toolname from a terminal, but I find the former is generally faster). This will provide the proper instructions for installing the package, including the name of a channel from which it should be installed. Here is an example that will install the requests Python package from the conda-forge channel. # install the requests Python package from the conda-forge channel conda install requests -c conda-forge Tip The channel conda-forge is special in that it contains very up-to-date versions of most common Python packages. In this way, it serves as an alternative to the standard channel (listing no channel at all). If you use conda-forge for some packages, it is generally recommended to use it for all of your packages, otherwise, you will continually have versions of standard Python dependencies being switched back and forth between super up-to-date versions from conda-forge and not quite as up-to-date versions from the standard channel. The easiest way to ensure that you don't forget to do this is to set conda-forge as your default channel , which you can do with the following command: conda config --add channels conda-forge","title":"Installing packages (using channels)"},{"location":"bootcamp/0.0-installation-conda/#creating-an-environment","text":"As mentioned already, you can create separate environments and install separate versions of software into them. As an example, let's create a new environment called 'test', install a package into it, and then remove it, since it is just a test example. This example also shows another important lesson: you can list multiple channels which will be searched in sorted order (like PATH) to find the package and its dependencies. Here I install raxml which will look first in the conda-forge and then bioconda channels. # create new env, activate, install softare, and call software conda create -n test conda activate test conda install raxml -c conda-forge -c bioconda raxmlHPC -v # switch back to the 'base' env and remove 'test' env conda activate base conda env remove -n test","title":"Creating an environment"},{"location":"bootcamp/0.0-installation-conda/#an-environment-for-this-textbook","text":"Finally, we've reached the point where you are ready to install all of the software associated with exercises in this textbook. As you might expect, I recommend installing this into a conda environment. This can be done easily using a special file called environment.yml , which is used to provide instructions to conda for installing many packages. example environment.yml file name : phylodatascience channels : - conda-forge - bioconda dependencies : - python>=3.7 - jupyter - numpy - pandas - scipy - numba - loguru - toytree>=3.0 - msprime>=1.0 - raxml # ...TODO You can get this file by cloning the git repository for this textbook. # clone the repo to a location (I use ~/Documents/) and cd into it. cd ~/Documents/ git clone https://github.com/eaton-lab/phylogenetic-data-science cd phylogenetic-data-science/ # create new env and install all dependencies into it. conda env create -f environment.yml # activate this environment conda activate phylodatascience","title":"An environment for this textbook"},{"location":"bootcamp/0.1-jupyter-and-binder/","text":"Introduction to Jupyter Notebooks \u00b6 Jupyter notebooks are a type of interactive document that combines executable code (usually Python, but many other languages are supported as well) with rendered text (Markdown/HTML) and other media. They serve as a powerful teaching tool for embedding lesson instructions alongside code that users edit and execute interactively. They are also widely used in research, and for sharing reproducible results and workflows. Jupyter servers \u00b6 But what is a jupyter notebook? Let's dive a bit deeper into how they work. Jupyter notebooks themselves are simply a document (a type of file with the suffix .ipynb ) that records the information that is entered into it. These notebooks are opened and run by servers , which are programs that send and receive information over a network. This could be a local network (just within your computer) or a public network (i.e., the internet). Jupyter servers work by connecting a notebook, as the frontend, to a Python kernel (a running session of Python), as the backend, communicating information back and forth between the two. This is the idea behind jupyter notebooks: A Python backend that uses a web-interface frontend to receive commands from users and send back results, which can then be saved to the notebook file. Most of the time , jupyter notebooks are run locally on a single computer. Below you can find instructions for installing and running jupyter locally. But, before we proceed to that, I want to introduce the concept of using remote servers to run jupyter. This is the process of starting a jupyter server on a remote machine, where all backend computation will take place, but connecting to it through a browser from your local machine. This may sound a bit complicated, and to some extent it is, but fortunately, for certain purposes, the process has become very streamlined through services such as Binder. Binder (remote server) \u00b6 Binder (mybinder.org) is a free web service that can be used to run a temporary jupyter server accessible from a public URI. Once it is setup you can simply share the link with anyone and when clicked it will quickly start a new server and open the notebooks for the user. It is a great way to share tutorials and lessons for teaching. You can even configure the servers to install and load the specific software versions that are required to run your code (this process is done using conda ). It has some limitations: the servers have only 2 processors available and a limited amount of disk space and RAM, but the amount is usually adequate for most simple tutorials. As a developer, it can be a great way to share examples of your code without requiring wary users to install anything new. We will use binder notebooks often throughout this textbook to share coding exercises. Using jupyter: A binder example \u00b6 The best way to learn how to use jupyter notebooks is to jump in and try one yourself. Click the following link to connect to a binder-hosted jupyter notebook. This notebook includes a lesson on the basics of editing Markdown and code blocks, switching between the two, executing code, and about other options available in notebooks: jupyter-intro.ipynb Installing jupyter locally \u00b6 Now that you are familiar with how notebooks work, you may wish to install jupyter locally on your machine so that you can get started using jupyter to learn Python and create notebooks of your own. Jupyter and all its dependencies can be easily installed with conda : conda install jupyter -c conda-forge Configuring jupyter \u00b6 Although it is not required, I recommend running a one-time configuration of jupyter which makes it a bit for convenient to use. Call the command below to create a jupyter configuration file which will be stored in a .jupyter/ dir in your home directory. jupyter-notebook --generate-config Now that this file exists you can mostly forget about it, unless you wish to customize your notebooks in some way. However, a benefit of it existing is that you can store a password for your server by running the command below. This is a measure that adds additional security, and also will stop jupyter from asking you to use random passwords. jupyter-notebook password Running jupyter locally \u00b6 Now we can start a jupyter notebook server with the following command. The option --no-browser is not necessary for everyone. For WSL2 users it will hide a warning message that would otherwise pop up telling you that it cannot open a browser automatically from your WLS2 system. Either way, once the server is running you can open a browser on your own and connect to it by entering the URI that will appear in your terminal. # move to your home directory, and start a notebook server from here. cd ~ jupyter-notebook --no-browser If you entered the option above then the notebook will not automatically open in your browser. You will simply see some text being printed in the terminal. Open a tab in your browser of choice and go to localhost:8888 . This tells your browser a local address which is receiving information from the local jupyter server running on your computer. Specifically, it is sending information to port number 8888, the default port used by jupyter. Jupyter filesystem interface \u00b6 As you have seen before, the jupyter interface will show you a view of your filesystem, including folder and files, from which you can either select an existing file to open, or create a new one. Jupyter notebooks are saved in a file format with the suffix .ipynb , and these will appear with little notebook icons next to them. Stopping the server \u00b6 When you start a jupyter server in a terminal, you can just minimize that terminal or stick it in a corner. The server will continue to function for as long as you have this server running. When you are done using, and have saved your notebooks, you can stop the server by closing it (or interrupt it by pressing ctrl-c ). Try this now by clicking on the terminal running the server and interrupting it. You can also close the browser tabs that were open. You can start the notebook server again at any time following the same commands be used above. Running a jupyter server remotely \u00b6 This is an advanced topic that you will likely not use yet as a new user. Although binder makes it easy to connect to a free server running in the cloud, the process of setting up jupyter to run on one of your own servers remotely is not that complicated. This is useful if, for example, you want to run notebooks from an HPC cluster. This type of connection of course requires much greater security than to run jupyter on a single machine, since you do not want just anybody to be able to connect to your jupyter server, which would give them nearly full access to your machine. The simple solution is to tunnel the connection through SSH: # a server on machine X serving on port 9999 jupyter-lab --ip = $( hostname ) --port = 9999 --no-browser # a user on machine Y securely connects to machine X:9999 # creating a tunnel that serves content to Y:9999 ssh -N -L 9999 :X:9999 user@hostname.hpc.edu The user would then leave this SSH tunnel open in a terminal on their laptop (machine Y) for as long as they want the tunnel connection to be established. The user could then open their browser to localhost:9999 and they should have access to the notebook server. The user can stop the SSH tunnel at any time by closing the terminal and the notebook server would continue to run, just with nobody connecting to it on the other end. When the user wishes to re-connect they simply need to start the SSH tunnel again and refresh their browser.","title":"0.1 - Python Jupyter/Binder"},{"location":"bootcamp/0.1-jupyter-and-binder/#introduction-to-jupyter-notebooks","text":"Jupyter notebooks are a type of interactive document that combines executable code (usually Python, but many other languages are supported as well) with rendered text (Markdown/HTML) and other media. They serve as a powerful teaching tool for embedding lesson instructions alongside code that users edit and execute interactively. They are also widely used in research, and for sharing reproducible results and workflows.","title":"Introduction to Jupyter Notebooks"},{"location":"bootcamp/0.1-jupyter-and-binder/#jupyter-servers","text":"But what is a jupyter notebook? Let's dive a bit deeper into how they work. Jupyter notebooks themselves are simply a document (a type of file with the suffix .ipynb ) that records the information that is entered into it. These notebooks are opened and run by servers , which are programs that send and receive information over a network. This could be a local network (just within your computer) or a public network (i.e., the internet). Jupyter servers work by connecting a notebook, as the frontend, to a Python kernel (a running session of Python), as the backend, communicating information back and forth between the two. This is the idea behind jupyter notebooks: A Python backend that uses a web-interface frontend to receive commands from users and send back results, which can then be saved to the notebook file. Most of the time , jupyter notebooks are run locally on a single computer. Below you can find instructions for installing and running jupyter locally. But, before we proceed to that, I want to introduce the concept of using remote servers to run jupyter. This is the process of starting a jupyter server on a remote machine, where all backend computation will take place, but connecting to it through a browser from your local machine. This may sound a bit complicated, and to some extent it is, but fortunately, for certain purposes, the process has become very streamlined through services such as Binder.","title":"Jupyter servers"},{"location":"bootcamp/0.1-jupyter-and-binder/#binder-remote-server","text":"Binder (mybinder.org) is a free web service that can be used to run a temporary jupyter server accessible from a public URI. Once it is setup you can simply share the link with anyone and when clicked it will quickly start a new server and open the notebooks for the user. It is a great way to share tutorials and lessons for teaching. You can even configure the servers to install and load the specific software versions that are required to run your code (this process is done using conda ). It has some limitations: the servers have only 2 processors available and a limited amount of disk space and RAM, but the amount is usually adequate for most simple tutorials. As a developer, it can be a great way to share examples of your code without requiring wary users to install anything new. We will use binder notebooks often throughout this textbook to share coding exercises.","title":"Binder (remote server)"},{"location":"bootcamp/0.1-jupyter-and-binder/#using-jupyter-a-binder-example","text":"The best way to learn how to use jupyter notebooks is to jump in and try one yourself. Click the following link to connect to a binder-hosted jupyter notebook. This notebook includes a lesson on the basics of editing Markdown and code blocks, switching between the two, executing code, and about other options available in notebooks: jupyter-intro.ipynb","title":"Using jupyter: A binder example"},{"location":"bootcamp/0.1-jupyter-and-binder/#installing-jupyter-locally","text":"Now that you are familiar with how notebooks work, you may wish to install jupyter locally on your machine so that you can get started using jupyter to learn Python and create notebooks of your own. Jupyter and all its dependencies can be easily installed with conda : conda install jupyter -c conda-forge","title":"Installing jupyter locally"},{"location":"bootcamp/0.1-jupyter-and-binder/#configuring-jupyter","text":"Although it is not required, I recommend running a one-time configuration of jupyter which makes it a bit for convenient to use. Call the command below to create a jupyter configuration file which will be stored in a .jupyter/ dir in your home directory. jupyter-notebook --generate-config Now that this file exists you can mostly forget about it, unless you wish to customize your notebooks in some way. However, a benefit of it existing is that you can store a password for your server by running the command below. This is a measure that adds additional security, and also will stop jupyter from asking you to use random passwords. jupyter-notebook password","title":"Configuring jupyter"},{"location":"bootcamp/0.1-jupyter-and-binder/#running-jupyter-locally","text":"Now we can start a jupyter notebook server with the following command. The option --no-browser is not necessary for everyone. For WSL2 users it will hide a warning message that would otherwise pop up telling you that it cannot open a browser automatically from your WLS2 system. Either way, once the server is running you can open a browser on your own and connect to it by entering the URI that will appear in your terminal. # move to your home directory, and start a notebook server from here. cd ~ jupyter-notebook --no-browser If you entered the option above then the notebook will not automatically open in your browser. You will simply see some text being printed in the terminal. Open a tab in your browser of choice and go to localhost:8888 . This tells your browser a local address which is receiving information from the local jupyter server running on your computer. Specifically, it is sending information to port number 8888, the default port used by jupyter.","title":"Running jupyter locally"},{"location":"bootcamp/0.1-jupyter-and-binder/#jupyter-filesystem-interface","text":"As you have seen before, the jupyter interface will show you a view of your filesystem, including folder and files, from which you can either select an existing file to open, or create a new one. Jupyter notebooks are saved in a file format with the suffix .ipynb , and these will appear with little notebook icons next to them.","title":"Jupyter filesystem interface"},{"location":"bootcamp/0.1-jupyter-and-binder/#stopping-the-server","text":"When you start a jupyter server in a terminal, you can just minimize that terminal or stick it in a corner. The server will continue to function for as long as you have this server running. When you are done using, and have saved your notebooks, you can stop the server by closing it (or interrupt it by pressing ctrl-c ). Try this now by clicking on the terminal running the server and interrupting it. You can also close the browser tabs that were open. You can start the notebook server again at any time following the same commands be used above.","title":"Stopping the server"},{"location":"bootcamp/0.1-jupyter-and-binder/#running-a-jupyter-server-remotely","text":"This is an advanced topic that you will likely not use yet as a new user. Although binder makes it easy to connect to a free server running in the cloud, the process of setting up jupyter to run on one of your own servers remotely is not that complicated. This is useful if, for example, you want to run notebooks from an HPC cluster. This type of connection of course requires much greater security than to run jupyter on a single machine, since you do not want just anybody to be able to connect to your jupyter server, which would give them nearly full access to your machine. The simple solution is to tunnel the connection through SSH: # a server on machine X serving on port 9999 jupyter-lab --ip = $( hostname ) --port = 9999 --no-browser # a user on machine Y securely connects to machine X:9999 # creating a tunnel that serves content to Y:9999 ssh -N -L 9999 :X:9999 user@hostname.hpc.edu The user would then leave this SSH tunnel open in a terminal on their laptop (machine Y) for as long as they want the tunnel connection to be established. The user could then open their browser to localhost:9999 and they should have access to the notebook server. The user can stop the SSH tunnel at any time by closing the terminal and the notebook server would continue to run, just with nobody connecting to it on the other end. When the user wishes to re-connect they simply need to start the SSH tunnel again and refresh their browser.","title":"Running a jupyter server remotely"},{"location":"bootcamp/0.2-python-types/","text":"Diving into Python types \u00b6 An introduction to basic types in Python is outside of the scope of this textbook. If needed, I refer readers to the official Python tutorial . Instead, this section will focus on describing differences among commonly used types and specifically why some types are used over others in certain situations. As this comes up in coding exercises in the textbook this section will be referred back to. Mutable versus Immutable \u00b6 When to use lists versus tuples. Most beginning Python programmers will use list objects much more commonly than tuple objects. Lists are mutable and Tuples are not. Elements can be replaced, added, or removed from lists. my_list = [ 1 , 2 , 'a' , 'b' ] my_list [ 0 ] = '1' my_list . remove ( 'a' ) Elements CANNOT be replaced, added, or removed from tuples. my_tup = ( 1 , 2 , 'a' , 'b' ) my_tup [ 0 ] = '1' # raises a TypeError my_tup = my_tup [: 3 ] # to mutate create a new tuple to replace previous one. Hashing, Sets and Dicts \u00b6 ... Arrays and DataFrames \u00b6 ... arr = np . array ([ 0 , 1 , 2 , 3 ]) arr = np . arange ( 4 ) arr = np . zeros ( 4 ) for idx in arr . shape [ 0 ]: arr [ idx ] = idx","title":"x.2 - Python types"},{"location":"bootcamp/0.2-python-types/#diving-into-python-types","text":"An introduction to basic types in Python is outside of the scope of this textbook. If needed, I refer readers to the official Python tutorial . Instead, this section will focus on describing differences among commonly used types and specifically why some types are used over others in certain situations. As this comes up in coding exercises in the textbook this section will be referred back to.","title":"Diving into Python types"},{"location":"bootcamp/0.2-python-types/#mutable-versus-immutable","text":"When to use lists versus tuples. Most beginning Python programmers will use list objects much more commonly than tuple objects. Lists are mutable and Tuples are not. Elements can be replaced, added, or removed from lists. my_list = [ 1 , 2 , 'a' , 'b' ] my_list [ 0 ] = '1' my_list . remove ( 'a' ) Elements CANNOT be replaced, added, or removed from tuples. my_tup = ( 1 , 2 , 'a' , 'b' ) my_tup [ 0 ] = '1' # raises a TypeError my_tup = my_tup [: 3 ] # to mutate create a new tuple to replace previous one.","title":"Mutable versus Immutable"},{"location":"bootcamp/0.2-python-types/#hashing-sets-and-dicts","text":"...","title":"Hashing, Sets and Dicts"},{"location":"bootcamp/0.2-python-types/#arrays-and-dataframes","text":"... arr = np . array ([ 0 , 1 , 2 , 3 ]) arr = np . arange ( 4 ) arr = np . zeros ( 4 ) for idx in arr . shape [ 0 ]: arr [ idx ] = idx","title":"Arrays and DataFrames"},{"location":"bootcamp/0.4-python-style-guide/","text":"Python Style Guide \u00b6 Python is a scripting language that is intended to be easy to read and write, and this underlies much of its popularity and growth over the last few decades. speed evolution interactivity jit-compilation data science revolution libraries machine learning Docstrings \u00b6 A documentation string (docstring) is used to provide information about a piece of code. For any public-facing part of your code, this should include sufficient description that a user can understand what the code is for, how to use it, and what to expect as a result. For private parts of your code, not intended to be viewed by users, docstrings are still incredibly useful. Online documentation \u00b6 Sphinx, mkdocs, and other tools can be used to build documentation for a package, including tutorials, etc. In fact this textbook was made using a version of mkdocs called mkdocs-material. Automated documentation is useful because you can make it update each time you make changes to the code. Automating documentation \u00b6 A benefit of automating documentation is that in addition to the documentation that you write yourself, using markdown or REST, parts of your documentation can also be build automatically by extracting information from the docstrings in your code. To do so, you must properly format the docstrings, and thus a number of standardized formats have been developed. These are useful not only because they can be parsed and understood by code, but following these practices will also make your documentation easier to understand, since users will recognize the common format it is written in. Sphinx style \u00b6 See example here or here. Numpy style \u00b6 Seee examples here. This is the style that I prefer, since it is more human readable than sphinx style, and can still be parsed by sphinx. Black style \u00b6 As with most languages, Python is flexible in allowing users to write code in various ways to accomplish the same task. Examples \u00b6 def get_num_nodes ( tree : toytree . Toytree ) -> int : \"\"\"Return the number of nodes in a tree. Example ------- >>> tree = toytree.rtree.rtree(10) >>> print(get_num_nodes(tree)) \"\"\" return len ( tree . idx_dict )","title":"Python Style Guide"},{"location":"bootcamp/0.4-python-style-guide/#python-style-guide","text":"Python is a scripting language that is intended to be easy to read and write, and this underlies much of its popularity and growth over the last few decades. speed evolution interactivity jit-compilation data science revolution libraries machine learning","title":"Python Style Guide"},{"location":"bootcamp/0.4-python-style-guide/#docstrings","text":"A documentation string (docstring) is used to provide information about a piece of code. For any public-facing part of your code, this should include sufficient description that a user can understand what the code is for, how to use it, and what to expect as a result. For private parts of your code, not intended to be viewed by users, docstrings are still incredibly useful.","title":"Docstrings"},{"location":"bootcamp/0.4-python-style-guide/#online-documentation","text":"Sphinx, mkdocs, and other tools can be used to build documentation for a package, including tutorials, etc. In fact this textbook was made using a version of mkdocs called mkdocs-material. Automated documentation is useful because you can make it update each time you make changes to the code.","title":"Online documentation"},{"location":"bootcamp/0.4-python-style-guide/#automating-documentation","text":"A benefit of automating documentation is that in addition to the documentation that you write yourself, using markdown or REST, parts of your documentation can also be build automatically by extracting information from the docstrings in your code. To do so, you must properly format the docstrings, and thus a number of standardized formats have been developed. These are useful not only because they can be parsed and understood by code, but following these practices will also make your documentation easier to understand, since users will recognize the common format it is written in.","title":"Automating documentation"},{"location":"bootcamp/0.4-python-style-guide/#sphinx-style","text":"See example here or here.","title":"Sphinx style"},{"location":"bootcamp/0.4-python-style-guide/#numpy-style","text":"Seee examples here. This is the style that I prefer, since it is more human readable than sphinx style, and can still be parsed by sphinx.","title":"Numpy style"},{"location":"bootcamp/0.4-python-style-guide/#black-style","text":"As with most languages, Python is flexible in allowing users to write code in various ways to accomplish the same task.","title":"Black style"},{"location":"bootcamp/0.4-python-style-guide/#examples","text":"def get_num_nodes ( tree : toytree . Toytree ) -> int : \"\"\"Return the number of nodes in a tree. Example ------- >>> tree = toytree.rtree.rtree(10) >>> print(get_num_nodes(tree)) \"\"\" return len ( tree . idx_dict )","title":"Examples"},{"location":"bootcamp/0.x-utilities/","text":"Useful development skills \u00b6 Measuring the speed/time of code execution \u00b6","title":"0.x utilities"},{"location":"bootcamp/0.x-utilities/#useful-development-skills","text":"","title":"Useful development skills"},{"location":"bootcamp/0.x-utilities/#measuring-the-speedtime-of-code-execution","text":"","title":"Measuring the speed/time of code execution"},{"location":"bootcamp/getting-started/","text":"Getting started \u00b6 This bootcamp tutorial is not intended to be a starting point for learning Python. There are many resources for that which are much more in depth (I recommend the official Python tutorial and documentation ). Instead, this bootcamp is intended for those who are already proficient at the command line, and familiar with Python, but may not have taken a formal programming class or had training in recent developments in Python. The goals of this bootcamp are to: provide a refresher on Python basics; provide a reference for methods that will be employed in this textbook; to reinforce style and uniformity that will make code easier to read and understand. Where to start \u00b6 If you wish to skip learning about software installation and jump right into learning Python coding then you can skip lesson 0.0 on software installation , and start with lesson 0.1 as an introduction to jupyter notebooks .","title":"Getting started"},{"location":"bootcamp/getting-started/#getting-started","text":"This bootcamp tutorial is not intended to be a starting point for learning Python. There are many resources for that which are much more in depth (I recommend the official Python tutorial and documentation ). Instead, this bootcamp is intended for those who are already proficient at the command line, and familiar with Python, but may not have taken a formal programming class or had training in recent developments in Python. The goals of this bootcamp are to: provide a refresher on Python basics; provide a reference for methods that will be employed in this textbook; to reinforce style and uniformity that will make code easier to read and understand.","title":"Getting started"},{"location":"bootcamp/getting-started/#where-to-start","text":"If you wish to skip learning about software installation and jump right into learning Python coding then you can skip lesson 0.0 on software installation , and start with lesson 0.1 as an introduction to jupyter notebooks .","title":"Where to start"},{"location":"chapter-1/","text":"Classes \u00b6 1.1: introduction Learning objectives What is computational phylogenetics? What is a phylogenetic tree? What does a tree represent: Many possible things. Examples What do we learn from phylogenetic trees: Biodiversity Systematics Epidemiology and forensics Function and homology What do we learn from collections of trees: Biogeography Demography and rates of change ... Trees as data, and data science. 1.2 Tree thinking What does tree thinking mean Terminology in trees Reading from the tips Rotations Rooting unrooted versus rooted what is a root node or edge changing the rooting changes the relationships Units ... Tree shapes topology Meta data edge lengths support values Trees as data: a visualization a model (depends on units and model of change) a variance-covariance matrix a relational object Saving trees newick format nexus format xml format other formats tree sequence table json Tree databases treebase Interpreting trees 1.3: Tree visualizations A history of visualizations Drawing (plotting) a tree The toytree Python library go to docs page Tree I/O Random trees Tree layout Edge lengths transform, show scale, ... Rooting, reroot Transform drop tips, Example: Load the mammal tree Prune a subclade Relabel, color, style, annotate. Exercises: nb-1.3: tree thinking, toytree, upham tree. reading: Felsenstein 34 1.4. Genealogy Wright-Fisher Process 1.4.1: Visualize the WF process Kingman coalescent 1.4.2: Infer Ne based on coalescent reps Coalescent simulations (msprime) 1.4.3: Simulate coalescent histories for a single pop Demographic models 1.4.4: Species Tree simulations (ipcoal) 1.4.5: Reading and Exercises 1.4 Mutations 4.0 - 'infinite-sites' mutations: 4.0-mutations.md 4.1 - Markov substitution models: 4.1-substituions.md 4.2 - Statistical properties of Markov models: 4.2-markov-models.md Reading and Exercises: 1-reading-list.md","title":"Index"},{"location":"chapter-1/#classes","text":"1.1: introduction Learning objectives What is computational phylogenetics? What is a phylogenetic tree? What does a tree represent: Many possible things. Examples What do we learn from phylogenetic trees: Biodiversity Systematics Epidemiology and forensics Function and homology What do we learn from collections of trees: Biogeography Demography and rates of change ... Trees as data, and data science. 1.2 Tree thinking What does tree thinking mean Terminology in trees Reading from the tips Rotations Rooting unrooted versus rooted what is a root node or edge changing the rooting changes the relationships Units ... Tree shapes topology Meta data edge lengths support values Trees as data: a visualization a model (depends on units and model of change) a variance-covariance matrix a relational object Saving trees newick format nexus format xml format other formats tree sequence table json Tree databases treebase Interpreting trees 1.3: Tree visualizations A history of visualizations Drawing (plotting) a tree The toytree Python library go to docs page Tree I/O Random trees Tree layout Edge lengths transform, show scale, ... Rooting, reroot Transform drop tips, Example: Load the mammal tree Prune a subclade Relabel, color, style, annotate. Exercises: nb-1.3: tree thinking, toytree, upham tree. reading: Felsenstein 34 1.4. Genealogy Wright-Fisher Process 1.4.1: Visualize the WF process Kingman coalescent 1.4.2: Infer Ne based on coalescent reps Coalescent simulations (msprime) 1.4.3: Simulate coalescent histories for a single pop Demographic models 1.4.4: Species Tree simulations (ipcoal) 1.4.5: Reading and Exercises 1.4 Mutations 4.0 - 'infinite-sites' mutations: 4.0-mutations.md 4.1 - Markov substitution models: 4.1-substituions.md 4.2 - Statistical properties of Markov models: 4.2-markov-models.md Reading and Exercises: 1-reading-list.md","title":"Classes"},{"location":"chapter-1/1.1-introduction/","text":"Introduction to Computational Phylogenetics \u00b6 Learning objectives \u00b6 This chapter will introduce the topic of trees as data objects. And it will act partly as a teaser to what is coming in this chapter, and in later chapters. This is big chapter that will probably not be ready for class v1. What type of data can be represented as trees? species/populations individuals genes alleles teasers coalescent links popgen to phylogenetics... mutations do not cover yet b/c it comes up next: common ancestry is the core concept? Computational phylogenetics \u00b6 The term phylogenetics refers to a broad array of scientific methods used ... and has evolved over time. It is sometimes viewed as separate from population genetics, which is concerned with changes over shorter evolutionary time scales. Despite major differences in scope, and many common assumptions, however, this distinction is mostly artificial. All evolutionary processes can be contextualized through the study of genealogical (tree-based) statistical heuristic algorithmic, Bayesian, machine learning, linear algebra, calculus. Phylogenetic algorithms \u00b6 traversal inheritance genetic drift (Brownian evolution) divergence substition models coalescence homoplasy convergence diversification What is a phylogenetic tree? \u00b6 What does a tree represent? \u00b6 A tree represents a hierarchical relationship. In the context of evolutionary biology these relationships are typically interpreted as a pattern of ancestry through time, i.e., ancestor-descendant relationships. Ancestor-descendant relationships \u00b6 genes and gene families individuals (genealogical) haplotypes (genealogical) population or species (model) what is a species? A concept? Then how can we model the relationships among conceptual things that we can't define precisely? However, this is not always the case, and thus to properly understand a tree it is important to understand what underlying data was used to infer the tree, and what type of inference method was used. Arbitrary representations of similarity \u00b6 Trees are used widely throughout mathematics, statistics, biology, and many other fields to represent any type of data that has a hierarchical structure, which includes pretty much any quantitative data for which pairwise distances can be measured. Such approaches are employed commonly in the social sciences to group samples by some quantitative measure of similarity. Examples include the study of human language families^1, of belief systems^2. Phylogenetic methods are thus not solely applicable to inferring evolutionary patterns. You will often see results displaying a figure that looks something like the one below. Let's consider an example like that from [this study] by XXX et al. (xxxx) , which used transcriptome sequencing to measure gene expression of different tissue types in a mouse and then performed UPGMA clustering to ... Chapter XXX covers distance-based phylogenetic inference methods in much more detail. Example code to create hierarchical tree from random data points import toytree import numpy as np from scipy import spatial , cluster # init a random number generator rng = np . random . default_rng ( seed = 123 ) # generate 100 random data points for 12 'samples' data = rng . random ( size = ( 12 , 100 )) # calculate a measure of distance between samples dists = toytree . distance . calculate_distances ( data , method = \"pdist\" ) # infer a tree by clustering samples by avg distance edges = toytree . distance . cluster_distances ( dists , method = \"average\" ) # translate edges into a tree tree = toytree . distance . get_tree_from_dists ( edges ) # draw the tree canvas , axes , mark = tree . draw () # draw a heatmap next to the tree mark2 = axes . table ( data ) A tree representing hierarchical relationships among arbitrary data looks similar to an evolutionary tree, but has a different meaning. Evolutionary models in phylogenetics \u00b6 How are evolutionary models distinct from other tree-based methods? Evolutionary models are probabilistic References \u00b6 https://github.com/mam288/Bioinformatics-IV","title":"x.1 - Introduction"},{"location":"chapter-1/1.1-introduction/#introduction-to-computational-phylogenetics","text":"","title":"Introduction to Computational Phylogenetics"},{"location":"chapter-1/1.1-introduction/#learning-objectives","text":"This chapter will introduce the topic of trees as data objects. And it will act partly as a teaser to what is coming in this chapter, and in later chapters. This is big chapter that will probably not be ready for class v1. What type of data can be represented as trees? species/populations individuals genes alleles teasers coalescent links popgen to phylogenetics... mutations do not cover yet b/c it comes up next: common ancestry is the core concept?","title":"Learning objectives"},{"location":"chapter-1/1.1-introduction/#computational-phylogenetics","text":"The term phylogenetics refers to a broad array of scientific methods used ... and has evolved over time. It is sometimes viewed as separate from population genetics, which is concerned with changes over shorter evolutionary time scales. Despite major differences in scope, and many common assumptions, however, this distinction is mostly artificial. All evolutionary processes can be contextualized through the study of genealogical (tree-based) statistical heuristic algorithmic, Bayesian, machine learning, linear algebra, calculus.","title":"Computational phylogenetics"},{"location":"chapter-1/1.1-introduction/#phylogenetic-algorithms","text":"traversal inheritance genetic drift (Brownian evolution) divergence substition models coalescence homoplasy convergence diversification","title":"Phylogenetic algorithms"},{"location":"chapter-1/1.1-introduction/#what-is-a-phylogenetic-tree","text":"","title":"What is a phylogenetic tree?"},{"location":"chapter-1/1.1-introduction/#what-does-a-tree-represent","text":"A tree represents a hierarchical relationship. In the context of evolutionary biology these relationships are typically interpreted as a pattern of ancestry through time, i.e., ancestor-descendant relationships.","title":"What does a tree represent?"},{"location":"chapter-1/1.1-introduction/#ancestor-descendant-relationships","text":"genes and gene families individuals (genealogical) haplotypes (genealogical) population or species (model) what is a species? A concept? Then how can we model the relationships among conceptual things that we can't define precisely? However, this is not always the case, and thus to properly understand a tree it is important to understand what underlying data was used to infer the tree, and what type of inference method was used.","title":"Ancestor-descendant relationships"},{"location":"chapter-1/1.1-introduction/#arbitrary-representations-of-similarity","text":"Trees are used widely throughout mathematics, statistics, biology, and many other fields to represent any type of data that has a hierarchical structure, which includes pretty much any quantitative data for which pairwise distances can be measured. Such approaches are employed commonly in the social sciences to group samples by some quantitative measure of similarity. Examples include the study of human language families^1, of belief systems^2. Phylogenetic methods are thus not solely applicable to inferring evolutionary patterns. You will often see results displaying a figure that looks something like the one below. Let's consider an example like that from [this study] by XXX et al. (xxxx) , which used transcriptome sequencing to measure gene expression of different tissue types in a mouse and then performed UPGMA clustering to ... Chapter XXX covers distance-based phylogenetic inference methods in much more detail. Example code to create hierarchical tree from random data points import toytree import numpy as np from scipy import spatial , cluster # init a random number generator rng = np . random . default_rng ( seed = 123 ) # generate 100 random data points for 12 'samples' data = rng . random ( size = ( 12 , 100 )) # calculate a measure of distance between samples dists = toytree . distance . calculate_distances ( data , method = \"pdist\" ) # infer a tree by clustering samples by avg distance edges = toytree . distance . cluster_distances ( dists , method = \"average\" ) # translate edges into a tree tree = toytree . distance . get_tree_from_dists ( edges ) # draw the tree canvas , axes , mark = tree . draw () # draw a heatmap next to the tree mark2 = axes . table ( data ) A tree representing hierarchical relationships among arbitrary data looks similar to an evolutionary tree, but has a different meaning.","title":"Arbitrary representations of similarity"},{"location":"chapter-1/1.1-introduction/#evolutionary-models-in-phylogenetics","text":"How are evolutionary models distinct from other tree-based methods? Evolutionary models are probabilistic","title":"Evolutionary models in phylogenetics"},{"location":"chapter-1/1.1-introduction/#references","text":"https://github.com/mam288/Bioinformatics-IV","title":"References"},{"location":"chapter-1/1.2-tree-thinking/","text":"Tree Thinking \u00b6 Follow along in a binder jupyter notebook: tree-thinking.ipynb . Understanding evolution \u00b6 The term tree thinking was coined by Baum, Smith, and Donovan 1 , and elaborated by Baum and Offner 2 , and Baum and Smith 3 , as a conceptual framework for teaching and studying evolution. It harkens to Darwin's description of evolution by natural selection as a process of \"descent with modification\" 4 by highlighting the importance of the \"tree-like\" structure of the evolutionary process -- tracing descendants back to their common ancestor -- as a way to communicate and understand evolution. WRITE IN A NOTEBOOK BELOW... \u00b6 Common ancestry \u00b6 Terminology \u00b6 Trees, undirected cyclic graph. Edges, branches, ... Nodes, vertices, ... Rotation \u00b6 Rooting \u00b6 Units \u00b6 What are the units of branch lengths in a tree? As we learned in chapter 1.1, trees can be used to represent many different things, and so there is no one answer for this. Tree shapes \u00b6 Special shapes: - caterpillar or ladder tree. - balanced tree - fan, comb, or unresolved tree. Random trees. - Tree shapes as data \u00b6 analysis of tree shapes using ABC for ... analysis of tree shapes for diversification ... This methods seem to have fallen out of favor, as they are less commonly employed as more powerful statistical methods have been developed for testing hypotheses about diversification rates (cite), and further statistical developments have highlighted challenges to simpler methods in distinguishing between alternative hypotheses for observed patterns of trees. References \u00b6 Baum, David A., Stacey DeWitt Smith, and Samuel S. S. Donovan. 2005. \u201cThe Tree-Thinking Challenge.\u201d Science 310 (5750): 979\u201380. https://doi.org/10.1126/science.1117727. \u21a9 Baum, David A., and Susan Offner. 2008. \u201cPhylogenics & Tree-Thinking.\u201d The American Biology Teacher 70 (4): 222\u201329. https://doi.org/10.1662/0002-7685(2008)70[222:PT]2.0.CO;2. \u21a9 Baum, David A., and Stacey D. Smith. 2012. Tree Thinking: An Introduction to Phylogenetic Biology. 1 st edition. Greenwood Village, Colo: W. H. Freeman. \u21a9 Darwin, Charles. 1859. On the Origin of Species by Means of Natural Selection, Or, The Preservation of Favoured Races in the Struggle for Life. J. Murray. \u21a9","title":"x.2 - Tree Thinking"},{"location":"chapter-1/1.2-tree-thinking/#tree-thinking","text":"Follow along in a binder jupyter notebook: tree-thinking.ipynb .","title":"Tree Thinking"},{"location":"chapter-1/1.2-tree-thinking/#understanding-evolution","text":"The term tree thinking was coined by Baum, Smith, and Donovan 1 , and elaborated by Baum and Offner 2 , and Baum and Smith 3 , as a conceptual framework for teaching and studying evolution. It harkens to Darwin's description of evolution by natural selection as a process of \"descent with modification\" 4 by highlighting the importance of the \"tree-like\" structure of the evolutionary process -- tracing descendants back to their common ancestor -- as a way to communicate and understand evolution.","title":"Understanding evolution"},{"location":"chapter-1/1.2-tree-thinking/#write-in-a-notebook-below","text":"","title":"WRITE IN A NOTEBOOK BELOW..."},{"location":"chapter-1/1.2-tree-thinking/#common-ancestry","text":"","title":"Common ancestry"},{"location":"chapter-1/1.2-tree-thinking/#terminology","text":"Trees, undirected cyclic graph. Edges, branches, ... Nodes, vertices, ...","title":"Terminology"},{"location":"chapter-1/1.2-tree-thinking/#rotation","text":"","title":"Rotation"},{"location":"chapter-1/1.2-tree-thinking/#rooting","text":"","title":"Rooting"},{"location":"chapter-1/1.2-tree-thinking/#units","text":"What are the units of branch lengths in a tree? As we learned in chapter 1.1, trees can be used to represent many different things, and so there is no one answer for this.","title":"Units"},{"location":"chapter-1/1.2-tree-thinking/#tree-shapes","text":"Special shapes: - caterpillar or ladder tree. - balanced tree - fan, comb, or unresolved tree. Random trees. -","title":"Tree shapes"},{"location":"chapter-1/1.2-tree-thinking/#tree-shapes-as-data","text":"analysis of tree shapes using ABC for ... analysis of tree shapes for diversification ... This methods seem to have fallen out of favor, as they are less commonly employed as more powerful statistical methods have been developed for testing hypotheses about diversification rates (cite), and further statistical developments have highlighted challenges to simpler methods in distinguishing between alternative hypotheses for observed patterns of trees.","title":"Tree shapes as data"},{"location":"chapter-1/1.2-tree-thinking/#references","text":"Baum, David A., Stacey DeWitt Smith, and Samuel S. S. Donovan. 2005. \u201cThe Tree-Thinking Challenge.\u201d Science 310 (5750): 979\u201380. https://doi.org/10.1126/science.1117727. \u21a9 Baum, David A., and Susan Offner. 2008. \u201cPhylogenics & Tree-Thinking.\u201d The American Biology Teacher 70 (4): 222\u201329. https://doi.org/10.1662/0002-7685(2008)70[222:PT]2.0.CO;2. \u21a9 Baum, David A., and Stacey D. Smith. 2012. Tree Thinking: An Introduction to Phylogenetic Biology. 1 st edition. Greenwood Village, Colo: W. H. Freeman. \u21a9 Darwin, Charles. 1859. On the Origin of Species by Means of Natural Selection, Or, The Preservation of Favoured Races in the Struggle for Life. J. Murray. \u21a9","title":"References"},{"location":"chapter-1/1.3-visualization/","text":"Tree Visualization \u00b6 Follow along in a binder jupyter notebook: tree-visualization.ipynb . A core utility of trees as data objects is for representing hierarchical relationships among samples, which is often interpreted through visualizations. This is the case for phylogenetic systematics is concerned with discerning the evolutionary relationships among clades of organisms. A history of tree visualization \u00b6 hand drawing Mesquite 1 , ape 2 , ggtree 3 , d3-web 4 . Drawing a tree \u00b6 The next two chapters will walk-through how to write a simple Tree class object in Python, and to develop a simple tree plotting function to draw a tree topology. This code will be overly simplified on purpose, but is similar in concept to the code underlying visualizations in any of the libraries described above. Before we dive into understanding the inner-workings of tree drawing algorithms, however, let's first walk through some examples to see what is possible using an established tree visualization library in Python, by using the package toytree . The toytree library \u00b6 Toytree 1 is a Throughout this textbook you will become very familiar with toytree , which was developed by authors of this textbook. References \u00b6 Felsenstein... \u21a9 \u21a9 Donoghue and Doyle... \u21a9 ete3... \u21a9 ape... \u21a9 ggtree... \u21a9","title":"x.3 - Visualization"},{"location":"chapter-1/1.3-visualization/#tree-visualization","text":"Follow along in a binder jupyter notebook: tree-visualization.ipynb . A core utility of trees as data objects is for representing hierarchical relationships among samples, which is often interpreted through visualizations. This is the case for phylogenetic systematics is concerned with discerning the evolutionary relationships among clades of organisms.","title":"Tree Visualization"},{"location":"chapter-1/1.3-visualization/#a-history-of-tree-visualization","text":"hand drawing Mesquite 1 , ape 2 , ggtree 3 , d3-web 4 .","title":"A history of tree visualization"},{"location":"chapter-1/1.3-visualization/#drawing-a-tree","text":"The next two chapters will walk-through how to write a simple Tree class object in Python, and to develop a simple tree plotting function to draw a tree topology. This code will be overly simplified on purpose, but is similar in concept to the code underlying visualizations in any of the libraries described above. Before we dive into understanding the inner-workings of tree drawing algorithms, however, let's first walk through some examples to see what is possible using an established tree visualization library in Python, by using the package toytree .","title":"Drawing a tree"},{"location":"chapter-1/1.3-visualization/#the-toytree-library","text":"Toytree 1 is a Throughout this textbook you will become very familiar with toytree , which was developed by authors of this textbook.","title":"The toytree library"},{"location":"chapter-1/1.3-visualization/#references","text":"Felsenstein... \u21a9 \u21a9 Donoghue and Doyle... \u21a9 ete3... \u21a9 ape... \u21a9 ggtree... \u21a9","title":"References"},{"location":"chapter-1/1.4-code-node-class/","text":"Code: An example Node and Tree class \u00b6 A tree can be represented in a variety of ways. Its representation can be visual, as we have seen already in drawings/plots of trees generated with graphical tools. But trees can also be represented as a data structure -- an object for storing information -- which is a widely used .. for trees. This is where computing comes in. How can we create a tree data structure, what does that even mean? Let's walk through it. A goal of representing a tree as a data structure is to be able to extract information from the tree. So let's start by trying to identify what information can or must exist in a tree. Start by looking at the tree drawing below and try to identify its core features. Example code to create simple tree figure # generate a random 8 tip ultrametric tree tree = toytree . rtree . unittree ( ntips = 8 , seed = 123 ) # draw the tree facing downwards with index node labels shown tree . draw ( layout = 'd' , node_labels = \"idx\" , node_sizes = 18 ) A tree representing hierarchical relationships among arbitrary data looks similar to an evolutionary tree, but has a different meaning. Example code to create a labeled tree figure # generate a random 8 tip ultrametric tree tree = toytree . rtree . unittree ( ntips = 8 , seed = 123 ) # draw the tree facing downward canvas , axes , mark = tree . draw ( layout = 'd' ) # add text annotations to the same coordinate axes mark2 = axes . text ( data ) # add line annotations to the same coordinate axes mark3 = axes . text ( data ) A tree representing hierarchical relationships among arbitrary data looks similar to an evolutionary tree, but has a different meaning. (1) Which Nodes connect to others (edges); (2) the lengths of these connections (edge lengths). This simplest form can be represented with a table, like the example below. Table data structure \u00b6 Storing trees as a table has potential benefits over alternative approaches. For one, a table is very memory efficient. If you have a tree with 1 million Nodes this could be stored in the space required for storing two columns of 1 million integers. This fact is taken advantage of in software tools like tskit , which is used to study collections of genealogical trees across entire genomes, which can include millions of trees of large size, where similarities among trees are recorded in the same table, rather than by duplicating their records. We will cover this in chapter XX. While tables can be efficient for storing and studying fixed trees, they are not the most efficient when the tree structure needs to be mutable, i.e., when we plan to modify the tree at various times by adding and/or removing Nodes from the tree. Depending on where the new Nodes are placed, we will need to increment the labels of each Node to update which ones are connected to which. Similarly, if we wanted to re-root the tree, or change other attributes such as the edge lengths. Example code get an edge table from a ToyTree # generate a random 8 tip ultrametric tree tree = toytree . rtree . unittree ( ntips = 8 , seed = 123 ) # get the tree edges as a table (DataFrame) and print print ( tree . get_edges ()) # OR canvas = toyplot . Canvas ( width = 800 , height = 400 ) table = canvas . table ( tree . get_edges (), bounds = ( \"5%\" , \"50%\" , \"5%\" , \"95%\" )) axes = canvas . cartesian ( bounds = ( \"50%\" , \"95%\" , \"5%\" , \"95%\" )) tree . draw ( axes = axes ); child parent 0 0 8 1 1 8 2 2 9 3 3 9 4 4 11 5 5 11 6 6 13 7 7 13 8 8 10 9 9 10 10 10 12 11 11 12 12 12 14 13 13 14 For example, say we wanted to find the names of all descendants of internal Node \"F\" in the tree above. Using a table this would multiple lookups in the table to ask which nodes are directly descended from \"F\", and then which are descended from those descendants. In Python, a table would likely be stored as an array, ndarray, or DataFrame, where such lookups are fast in absolute terms, but still slow relative to an alternative approach. Tree data structure \u00b6 An alternative is to store the Nodes as a Tree data structure. In general terms, this can be done in any object-oriented programming language where arbitrary class objects can be created in memory, and references among these objects can also be stored. This approach to object-oriented programming (OOP) is widely used, especially in languages like C, Java, and Python. Doing so requires tracing the connections among nodes, from one to the next, in a certain order. This process, termed a tree traversal , turns out to be a very common algorithm used throughout computer science. Consequently, the development of tree data structures have been well studied. For example, the file system on your computer represents a tree data structure, where folders are nested within folders nested within folders. File browsers and other computer programs traverse the file system frequently, for example when looking for files of a certain name or type. This process could be accomplished by storing the names and types of all files in memory at once, however, it is much more efficient to simply look inside each subfolder at a time, as it is visited, during the traversal process. Moreover, if you have some idea of where the file you are looking for is located, you can start the search from that location and only files descended (nested within) that location will be searched. We will find that traversal strategies are a core concept in developing and using tree-based algorithms. This makes them not only central to certain computational methods, but also central to many algorithms used in evolutionary biology. Coding practices \u00b6 Because this is our coding exercise we will start slowly, describing the steps that are taken and why. This lesson will use concepts from the bootcamp chapter including types , type hints , and Object oriented programming . OOP \u00b6 Let's develop a Tree class object. Here we will benefit from object oriented programming, and this lesson will serve to introduce some core concepts of OOP. A tree is composed of Nodes and Edges. These could each be represented by an object in memory, however, a more common approach when working with tree or graph data structures is to simply represent Nodes in memory, and for Edges to represent the relationships among Node objects. Node class 1 \u00b6 We will walkthrough the development of a Node class object for storing information in a tree data structure. To describe this development we will start with a very simple object and then iteratively add to it to build up to a more complex Python class. What is a Node? A single Node of a tree can either be a terminal, internal, or root Node. A terminal node has no descendants, this is sometimes referred to its degree=1 (reference), because it has one edge in (its parent), and no edges out (no descendants). An internal Node by contrast has degree >= 2, since it has a parent in addition to one or more descendants. Finally, a root node can have degree >= 2, but it does not have a parent edge, only descendants. Each of these can be represented by a class object that simply store other objects as its parent and children. Let's start with a simple example. Each NodeExample class object has a name and dist associated with it. class Node1 : def __init__ ( self , name , dist ): # store parameters self . name = name self . dist = dist # default attributes of Node objects self . children = () self . up = None This class is very simple, and so not yet very useful. But already we can begin to create a tree data structure simply by creating connections among Node objects. Below we create three Node instances with different names, and then edit the .children or .up attributes to create references (pointers) from one to another, such that the node named \"A\" is parent to Nodes \"B\" and \"C\". # create several Node objects node_a = Node1 ( name = \"A\" , dist = 1.0 ) node_b = Node1 ( name = \"B\" , dist = 1.0 ) node_c = Node1 ( name = \"C\" , dist = 1.0 ) # connect them to setting their children and/or up attributes node_a . children = ( node_b , node_c ) node_b . up = node_a node_c . up = node_a There is now information not only within each class instance, but there is also information we can extract about the relationships among them. Below this is done within a for-loop visiting each node to print some information from its attributes: # now each Node has data (information about itself and its connections) for node in ( node1 , node2 , node3 ): children_names = [ i . name for i in node . children ] parent_name = node . up . name if node . up else \"None\" print ( f \" { node . name } , children= { children_names } \" , up = { parent_name }) A, children=[B, C], up=None B, children=[], up=A C, children=[], up=A Node class 2 \u00b6 Now let's make a more proper version this class. This should include a well formatted docstring, and type hints for all parameters and attributes of the class. Here we import an interesting new extension called annotations which allows us to reference the new class we are defining as a type hint for itself (i.e., the Node class children are also Node class objects). from __future__ import annotations from typing import Tuple , Optional class NodeBase : \"\"\"A Node instance that can connect with other Nodes to form a Tree. Parameters ---------- name: str A name string associated with a Node when printed or visualized. dist: float A float value as the distance between this Node and its parent (up) Attributes ---------- children: Tuple A tuple of Node instances that are descended from this Node. up: Node or None A Node that is ancestral to this Node, or None if this Node is root. \"\"\" def __init__ ( self , name : str = \"\" , dist : float = 0. ): self . name = str ( name ) self . dist = float ( dist ) self . children : Tuple [ Node ] = () self . up : Optional [ Node ] = None def __repr__ ( self ) -> str : \"\"\"Return string representation of the Node relationship\"\"\" children = [ i . name for i in self . children ] parent = self . up . name if self . up else 'None' return f \"<Node { self . name } , up= { parent } , children= { children } >\" def is_leaf ( self ) -> bool : \"\"\"Return True if Node is a leaf (i.e., no children)\"\"\" return bool ( self . children ) def is_root ( self ) -> bool : \"\"\"Return True if Node is the root.\"\"\" return self . up is None def add_child ( self , node : Node ) -> None : \"\"\"Add a Node as a child to this one.\"\"\" node . up = self self . children += ( node ,) node_a = Node ( \"a\" ) node_b = Node ( \"b\" ) node_c = Node ( \"c\" ) Traversal \u00b6 A key feature of tree objects is the process of traversal, by which each Node is visited in some determined order. Traversal algorithms make it possible to calculate information on trees fast and efficiently, typically by performing calculations on parts of the tree which can be used for later calculations. Examples of this include Felsenstein's pruning algorithm , which we will cover later, as an efficient traversal algorithm for calculating parsimony or likelihood scores on trees (citation). Here we will first cover a more simple example, showing how traversal provides an efficient method for calculating the Node coordinate layout for plotting trees. As a toy example let's extend our Node class object by creating a superclass -- a class that inherits the properties of the Node class, but also contains additional attributes or functions that are defined. Here we are using the class extending only as a teaching tool, to break up the description of this class into smaller chunks. However, we will later see examples where ... This class will have the same __init__ as the NodeBase, meaning that it also takes an optional name and dist values. from typing import Generator from collections import deque class Node ( NodeBase ): \"\"\"Tree is a superclass of Node with traversal functions. \"\"\" def _traverse_in_root_to_tip_order ( self ) -> Generator : \"\"\"Yield all nodes in order of n descendants from root to tips.\"\"\" # start with root in queue. When queue is empty traversal is finished. queue = deque ([ self ]) while queue : # return the left-most node in the queue. yield returns current # state while allowing the func to resume (i.e., a generator). node = queue . popleft () yield node # append the node's children to the right end of the queue queue . extend ( node . children ) def _traverse_in_tips_to_root_order ( self ) -> Generator : \"\"\"Yield all nodes in order by visiting children before parents.\"\"\" stack1 = deque ([ self ]) stack2 = deque () while stack1 : # pop a node from stack1 and append to stack2 node = stack1 . pop () stack2 . append ( node ) # append left and right of current node to stack1 stack1 . append () if not node . is_leaf (): queue . extend () else : yield node def _traverse_parents_then_children ( self ) -> Generator : \"\"\"Yield all nodes in order of parents before children.\"\"\" queue = deque () node = self while node is not None : # attach current node's children to front of queue queue . extendleft ( node . children [:: - 1 ]) # return the current node yield node # update node from left of queue try : node = queue . popleft () except IndexError : node = None Create a simple tree \u00b6 This is a simplified version of a function we used in the last lesson to generate a random tree of connected Nodes in toytree , using the function toytree.rtree.imbtree . The function below will return an imbalanced tree by starting with a root Node and adding pairs of children iteratively to the right descendant Node until the number of connected tip Nodes reaches the requested number (ntips). It also extends the dist attribute of the left child Node each iteration def ladder_tree ( ntips : int ): \"\"\"Return a ladder-like tree of Node objects.\"\"\" # create root Node and select it as current focal node node = root = Node ( name = \"root\" ) for idx in range ( 0 , ntips , 2 ): # add two children to the focal node node . add_child ( Node ( idx , dist = 1 )) node . add_child ( Node ( idx + 1 , dist = 1 )) # make right child the new focal node node = node . children [ 1 ] return root for node in","title":"x.4 - Code - Node class"},{"location":"chapter-1/1.4-code-node-class/#code-an-example-node-and-tree-class","text":"A tree can be represented in a variety of ways. Its representation can be visual, as we have seen already in drawings/plots of trees generated with graphical tools. But trees can also be represented as a data structure -- an object for storing information -- which is a widely used .. for trees. This is where computing comes in. How can we create a tree data structure, what does that even mean? Let's walk through it. A goal of representing a tree as a data structure is to be able to extract information from the tree. So let's start by trying to identify what information can or must exist in a tree. Start by looking at the tree drawing below and try to identify its core features. Example code to create simple tree figure # generate a random 8 tip ultrametric tree tree = toytree . rtree . unittree ( ntips = 8 , seed = 123 ) # draw the tree facing downwards with index node labels shown tree . draw ( layout = 'd' , node_labels = \"idx\" , node_sizes = 18 ) A tree representing hierarchical relationships among arbitrary data looks similar to an evolutionary tree, but has a different meaning. Example code to create a labeled tree figure # generate a random 8 tip ultrametric tree tree = toytree . rtree . unittree ( ntips = 8 , seed = 123 ) # draw the tree facing downward canvas , axes , mark = tree . draw ( layout = 'd' ) # add text annotations to the same coordinate axes mark2 = axes . text ( data ) # add line annotations to the same coordinate axes mark3 = axes . text ( data ) A tree representing hierarchical relationships among arbitrary data looks similar to an evolutionary tree, but has a different meaning. (1) Which Nodes connect to others (edges); (2) the lengths of these connections (edge lengths). This simplest form can be represented with a table, like the example below.","title":"Code: An example Node and Tree class"},{"location":"chapter-1/1.4-code-node-class/#table-data-structure","text":"Storing trees as a table has potential benefits over alternative approaches. For one, a table is very memory efficient. If you have a tree with 1 million Nodes this could be stored in the space required for storing two columns of 1 million integers. This fact is taken advantage of in software tools like tskit , which is used to study collections of genealogical trees across entire genomes, which can include millions of trees of large size, where similarities among trees are recorded in the same table, rather than by duplicating their records. We will cover this in chapter XX. While tables can be efficient for storing and studying fixed trees, they are not the most efficient when the tree structure needs to be mutable, i.e., when we plan to modify the tree at various times by adding and/or removing Nodes from the tree. Depending on where the new Nodes are placed, we will need to increment the labels of each Node to update which ones are connected to which. Similarly, if we wanted to re-root the tree, or change other attributes such as the edge lengths. Example code get an edge table from a ToyTree # generate a random 8 tip ultrametric tree tree = toytree . rtree . unittree ( ntips = 8 , seed = 123 ) # get the tree edges as a table (DataFrame) and print print ( tree . get_edges ()) # OR canvas = toyplot . Canvas ( width = 800 , height = 400 ) table = canvas . table ( tree . get_edges (), bounds = ( \"5%\" , \"50%\" , \"5%\" , \"95%\" )) axes = canvas . cartesian ( bounds = ( \"50%\" , \"95%\" , \"5%\" , \"95%\" )) tree . draw ( axes = axes ); child parent 0 0 8 1 1 8 2 2 9 3 3 9 4 4 11 5 5 11 6 6 13 7 7 13 8 8 10 9 9 10 10 10 12 11 11 12 12 12 14 13 13 14 For example, say we wanted to find the names of all descendants of internal Node \"F\" in the tree above. Using a table this would multiple lookups in the table to ask which nodes are directly descended from \"F\", and then which are descended from those descendants. In Python, a table would likely be stored as an array, ndarray, or DataFrame, where such lookups are fast in absolute terms, but still slow relative to an alternative approach.","title":"Table data structure"},{"location":"chapter-1/1.4-code-node-class/#tree-data-structure","text":"An alternative is to store the Nodes as a Tree data structure. In general terms, this can be done in any object-oriented programming language where arbitrary class objects can be created in memory, and references among these objects can also be stored. This approach to object-oriented programming (OOP) is widely used, especially in languages like C, Java, and Python. Doing so requires tracing the connections among nodes, from one to the next, in a certain order. This process, termed a tree traversal , turns out to be a very common algorithm used throughout computer science. Consequently, the development of tree data structures have been well studied. For example, the file system on your computer represents a tree data structure, where folders are nested within folders nested within folders. File browsers and other computer programs traverse the file system frequently, for example when looking for files of a certain name or type. This process could be accomplished by storing the names and types of all files in memory at once, however, it is much more efficient to simply look inside each subfolder at a time, as it is visited, during the traversal process. Moreover, if you have some idea of where the file you are looking for is located, you can start the search from that location and only files descended (nested within) that location will be searched. We will find that traversal strategies are a core concept in developing and using tree-based algorithms. This makes them not only central to certain computational methods, but also central to many algorithms used in evolutionary biology.","title":"Tree data structure"},{"location":"chapter-1/1.4-code-node-class/#coding-practices","text":"Because this is our coding exercise we will start slowly, describing the steps that are taken and why. This lesson will use concepts from the bootcamp chapter including types , type hints , and Object oriented programming .","title":"Coding practices"},{"location":"chapter-1/1.4-code-node-class/#oop","text":"Let's develop a Tree class object. Here we will benefit from object oriented programming, and this lesson will serve to introduce some core concepts of OOP. A tree is composed of Nodes and Edges. These could each be represented by an object in memory, however, a more common approach when working with tree or graph data structures is to simply represent Nodes in memory, and for Edges to represent the relationships among Node objects.","title":"OOP"},{"location":"chapter-1/1.4-code-node-class/#node-class-1","text":"We will walkthrough the development of a Node class object for storing information in a tree data structure. To describe this development we will start with a very simple object and then iteratively add to it to build up to a more complex Python class. What is a Node? A single Node of a tree can either be a terminal, internal, or root Node. A terminal node has no descendants, this is sometimes referred to its degree=1 (reference), because it has one edge in (its parent), and no edges out (no descendants). An internal Node by contrast has degree >= 2, since it has a parent in addition to one or more descendants. Finally, a root node can have degree >= 2, but it does not have a parent edge, only descendants. Each of these can be represented by a class object that simply store other objects as its parent and children. Let's start with a simple example. Each NodeExample class object has a name and dist associated with it. class Node1 : def __init__ ( self , name , dist ): # store parameters self . name = name self . dist = dist # default attributes of Node objects self . children = () self . up = None This class is very simple, and so not yet very useful. But already we can begin to create a tree data structure simply by creating connections among Node objects. Below we create three Node instances with different names, and then edit the .children or .up attributes to create references (pointers) from one to another, such that the node named \"A\" is parent to Nodes \"B\" and \"C\". # create several Node objects node_a = Node1 ( name = \"A\" , dist = 1.0 ) node_b = Node1 ( name = \"B\" , dist = 1.0 ) node_c = Node1 ( name = \"C\" , dist = 1.0 ) # connect them to setting their children and/or up attributes node_a . children = ( node_b , node_c ) node_b . up = node_a node_c . up = node_a There is now information not only within each class instance, but there is also information we can extract about the relationships among them. Below this is done within a for-loop visiting each node to print some information from its attributes: # now each Node has data (information about itself and its connections) for node in ( node1 , node2 , node3 ): children_names = [ i . name for i in node . children ] parent_name = node . up . name if node . up else \"None\" print ( f \" { node . name } , children= { children_names } \" , up = { parent_name }) A, children=[B, C], up=None B, children=[], up=A C, children=[], up=A","title":"Node class 1"},{"location":"chapter-1/1.4-code-node-class/#node-class-2","text":"Now let's make a more proper version this class. This should include a well formatted docstring, and type hints for all parameters and attributes of the class. Here we import an interesting new extension called annotations which allows us to reference the new class we are defining as a type hint for itself (i.e., the Node class children are also Node class objects). from __future__ import annotations from typing import Tuple , Optional class NodeBase : \"\"\"A Node instance that can connect with other Nodes to form a Tree. Parameters ---------- name: str A name string associated with a Node when printed or visualized. dist: float A float value as the distance between this Node and its parent (up) Attributes ---------- children: Tuple A tuple of Node instances that are descended from this Node. up: Node or None A Node that is ancestral to this Node, or None if this Node is root. \"\"\" def __init__ ( self , name : str = \"\" , dist : float = 0. ): self . name = str ( name ) self . dist = float ( dist ) self . children : Tuple [ Node ] = () self . up : Optional [ Node ] = None def __repr__ ( self ) -> str : \"\"\"Return string representation of the Node relationship\"\"\" children = [ i . name for i in self . children ] parent = self . up . name if self . up else 'None' return f \"<Node { self . name } , up= { parent } , children= { children } >\" def is_leaf ( self ) -> bool : \"\"\"Return True if Node is a leaf (i.e., no children)\"\"\" return bool ( self . children ) def is_root ( self ) -> bool : \"\"\"Return True if Node is the root.\"\"\" return self . up is None def add_child ( self , node : Node ) -> None : \"\"\"Add a Node as a child to this one.\"\"\" node . up = self self . children += ( node ,) node_a = Node ( \"a\" ) node_b = Node ( \"b\" ) node_c = Node ( \"c\" )","title":"Node class 2"},{"location":"chapter-1/1.4-code-node-class/#traversal","text":"A key feature of tree objects is the process of traversal, by which each Node is visited in some determined order. Traversal algorithms make it possible to calculate information on trees fast and efficiently, typically by performing calculations on parts of the tree which can be used for later calculations. Examples of this include Felsenstein's pruning algorithm , which we will cover later, as an efficient traversal algorithm for calculating parsimony or likelihood scores on trees (citation). Here we will first cover a more simple example, showing how traversal provides an efficient method for calculating the Node coordinate layout for plotting trees. As a toy example let's extend our Node class object by creating a superclass -- a class that inherits the properties of the Node class, but also contains additional attributes or functions that are defined. Here we are using the class extending only as a teaching tool, to break up the description of this class into smaller chunks. However, we will later see examples where ... This class will have the same __init__ as the NodeBase, meaning that it also takes an optional name and dist values. from typing import Generator from collections import deque class Node ( NodeBase ): \"\"\"Tree is a superclass of Node with traversal functions. \"\"\" def _traverse_in_root_to_tip_order ( self ) -> Generator : \"\"\"Yield all nodes in order of n descendants from root to tips.\"\"\" # start with root in queue. When queue is empty traversal is finished. queue = deque ([ self ]) while queue : # return the left-most node in the queue. yield returns current # state while allowing the func to resume (i.e., a generator). node = queue . popleft () yield node # append the node's children to the right end of the queue queue . extend ( node . children ) def _traverse_in_tips_to_root_order ( self ) -> Generator : \"\"\"Yield all nodes in order by visiting children before parents.\"\"\" stack1 = deque ([ self ]) stack2 = deque () while stack1 : # pop a node from stack1 and append to stack2 node = stack1 . pop () stack2 . append ( node ) # append left and right of current node to stack1 stack1 . append () if not node . is_leaf (): queue . extend () else : yield node def _traverse_parents_then_children ( self ) -> Generator : \"\"\"Yield all nodes in order of parents before children.\"\"\" queue = deque () node = self while node is not None : # attach current node's children to front of queue queue . extendleft ( node . children [:: - 1 ]) # return the current node yield node # update node from left of queue try : node = queue . popleft () except IndexError : node = None","title":"Traversal"},{"location":"chapter-1/1.4-code-node-class/#create-a-simple-tree","text":"This is a simplified version of a function we used in the last lesson to generate a random tree of connected Nodes in toytree , using the function toytree.rtree.imbtree . The function below will return an imbalanced tree by starting with a root Node and adding pairs of children iteratively to the right descendant Node until the number of connected tip Nodes reaches the requested number (ntips). It also extends the dist attribute of the left child Node each iteration def ladder_tree ( ntips : int ): \"\"\"Return a ladder-like tree of Node objects.\"\"\" # create root Node and select it as current focal node node = root = Node ( name = \"root\" ) for idx in range ( 0 , ntips , 2 ): # add two children to the focal node node . add_child ( Node ( idx , dist = 1 )) node . add_child ( Node ( idx + 1 , dist = 1 )) # make right child the new focal node node = node . children [ 1 ] return root for node in","title":"Create a simple tree"},{"location":"chapter-1/1.5-code-tree-class/","text":"The toytree classes \u00b6 The last lesson demonstrated how to develop a simple tree structure by connecting multiple Node class objects. In this chapter we will focus on the development of a class object that is closer in design to what is used in the toytree package. This will help to illuminate how to use toytree more effectively, while also demonstrating some coding practices that can be useful in many other contexts. Learning objectives \u00b6 learn toytree.Node and toytree.ToyTree classes. learn about designing Classes using the Python @property decorator and private attributes. learn about caching and speed Python loves cached data \u00b6 Traversals are relatively fast, however, for some types of analyses there are advantages to storing cached (saved to hash table) information that can be retrieved without requiring traversal of the tree. This is expecially useful if the tree is very large and you want to extract data from only a single node. One example of this is Node heights. Calculating heights seems simple, it is just the sum of edge lengths from the root Node, right? Well, in an ultrametric tree that would be true, however, not all trees are ultrametric. If tips are different distances from the root, then the height of a Node may need to be calculated relative to some other connected Node that is farthest from the root. This requires calculating the distance of every tip from the root, and then asking how far above some other tip your focal tip is located. Not too complicated, but the ability to do this quickly can speed up many operations. Remember from the [Coding Bootcamp] that we can measure the speed of executed code easily using the IPython magic command %timeit tree = toytree . rtree . unittree ( ntips = 10 , seed = 123 ) def get_ntips_example1 ( tree : toytree . ToyTree ) -> int : \"\"\"Return leaf Nodes by traversing tree.\"\"\" return sum ( 1 if i . is_leaf () for i in tree . traverse ()) def get_ntips_example2 ( tree : toytree . ToyTree ) -> int : \"\"\"Return leaf Nodes from cache.\"\"\" return tree . ntips %% timeit Private attributes \u00b6 These are termed private or hidden because they are not intended to be accessed by a user. The single underscore at the beginning of their name indicates this; stay away. It is further exemplified within interactive environments, like a jupyter notebook, where using tab-completion to view attributes and functions associated with an object will not show private attributes and functions by default, unless you begin to type the underscore. Why create private attributes and functions? There are actually several good reasons: (1) to only show users the code that they are intended to use; (2) to limit or specify how a user can modify an object; and (3) to limit to use. This can require a lot of careful work by the developers, and is often overlooked, but has a huge impact on making a package easy to learn and use. toytree.Node first look \u00b6 Let's start by creating an example Node object that uses the @property decorator, and which only defines private attribute names in the __init__ . Remember that all attributes of a class should be defined in the __init__ function. So how will a user access data for this Node is all of its attributes are private? The answer is to use properties, which is easiest to do using the @property decorator. The user will now be able to see NodeExample1.name , which looks like an attribute, but is actually a property. It will return the _name attribute when called, and will call the associated setter function if the user tries to assign a new value to the property. This latter usage is perhaps the most important. It provides an opportunity to intercept the value that the user wants to assign to the property and to check it for various requirements before assigning it to the associated private attribute. For example, if a user tried to call node.name = 3 the setter function here intercepts the input value and converts it to a string by using the str() function before assigning it to the node._name attribute. class Node1 : \"\"\"A simplified version of toytree.Node used here for demonstration.\"\"\" def __init__ ( self , name : str = \"\" ): self . _name = name # non-init attributes for storing connections among Nodes self . up : Node = None self . children : Tuple [ 'Node1' ] = () @property def name ( self ) -> str : return self . _name @name . setter def name ( self , value : str ) -> None : self . _name = str ( value ) So we now have an improved method for setting the name of a Node, which will ensure that the value is always a string. What else can we use properties for? In toytree we use this framework to purposefully make Node objects immutable, meaning that do not want users to edit the attributes of Node's directly, for example by editing which Node is the parent of another. An example of this would be the following: node1 = Node1 ( '1' ) node2 = Node1 ( '2' ) node3 = Node1 ( '3' ) node1 . children = [ node2 , node3 ] The above operation is allowed using the simple Node class that we developed in the previous lesson. However, this is not a good operation, since the references among the objects is not complete. node1 has a reference to its two child Nodes, however, the child nodes do did not set node1 as their parent ( ._up attribute). One way to restrict users from making this error is to only allow them to modify .children or .up using functions that are designed to ensure that they are set appropriately. class Node2 : \"\"\"A simplified version of toytree.Node used here for demonstration.\"\"\" def __init__ ( self , name : str = \"\" ): self . _name = name # non-init attributes for storing connections among Nodes self . _children : Tuple [ Node ] = () self . _up : Optional [ Node ] = None @property def name ( self ) -> str : return self . _name @name . setter def name ( self , value : str ) -> None : self . _name = str ( value ) @property def children ( self ) -> Tuple [ Node ]: return self . _children @children . setter def children ( self , value : Tuple [ Node ]) -> None : raise TypeError ( \"'children' attribute does not allow item assignment\" ) @property def up ( self ) -> Node : return self . _up @up . setter def up ( self , value : Tuple [ Node ]) -> None : raise TypeError ( \"'up' attribute does not allow item assignment\" ) def add_child ( self , node : Node ) -> None : \"\"\"Add a Node as a connected child to this Node.\"\"\" node . _up = self self . _children += ( node , ) Now to assign a child the user must use the add_child function, which ensures that when a child is added the connections are made from both parent to child, and child to parent. If the user has an advanced understanding of the Node2 class they can of course work around this solution by modifying the hidden ._up and _children attributes, but in that case they know that they are editing attributes that were purposefully not exposed to the user, and so should not be surprised if they potentially encounter unexpected outcomes. Private functions \u00b6 We may similarly wish to develop private functions. This is useful for functions that are for internal use by the developer, but are unlikely to be useful to users. By hiding them from the user it will present a more user-friendly object that directs them to use it in the best way possible. An immutable Node class \u00b6 Have a look at the toytree.Node class object from the current main branch of toytree on GitHub. You can see that it follows the principle we can developed above but with a few additional private attributes. Most importantly, this includes the .height attribute of a Node. Unique node labels (idx) \u00b6 A set of connected Nodes can only be accessed by their traversal currently, or by data assigned to nodes, such as name attributes. We have learned how to use properties to limit how users assign values to a single object, but what if we want to assign data to a Node that takes into account the data of all other connected Nodes? An example would be assigning a unique number to every Node to serve as a label. ... The ToyTree class object \u00b6 As we've discussed previously, a tree-like structure can be represented simply by the connections among Node objects, so what use is there in creating a separate Tree class object? In toytree this was design choice was made because we decided there was a distinction in the intended way that a Tree versus Node object is intended to be used. A Node stores data including information about its direct edges, whereas a Tree stores information about a set of connected Nodes, and has functions meant for operating on them as a collection. With this in mind, we created a class object called ToyTree that is initialized with a Node object that represents the root Node of a tree (whether or not the tree is rooted). # design expectation tree = ToyTree ( root_node ) # access information about all Nodes connected to root_node print ( tree . nnodes ) print ( tree . ntips ) print ( tree . get_node_data ( \"height\" )) # traverse Nodes in desired order # edit Node attributes (returns a new tree with modified Node dists) tree = tree . set_node_data ( \"height\" , mapping = { 0 : 100 , 1 : 200 })","title":"x.5 - Code - Tree class"},{"location":"chapter-1/1.5-code-tree-class/#the-toytree-classes","text":"The last lesson demonstrated how to develop a simple tree structure by connecting multiple Node class objects. In this chapter we will focus on the development of a class object that is closer in design to what is used in the toytree package. This will help to illuminate how to use toytree more effectively, while also demonstrating some coding practices that can be useful in many other contexts.","title":"The toytree classes"},{"location":"chapter-1/1.5-code-tree-class/#learning-objectives","text":"learn toytree.Node and toytree.ToyTree classes. learn about designing Classes using the Python @property decorator and private attributes. learn about caching and speed","title":"Learning objectives"},{"location":"chapter-1/1.5-code-tree-class/#python-loves-cached-data","text":"Traversals are relatively fast, however, for some types of analyses there are advantages to storing cached (saved to hash table) information that can be retrieved without requiring traversal of the tree. This is expecially useful if the tree is very large and you want to extract data from only a single node. One example of this is Node heights. Calculating heights seems simple, it is just the sum of edge lengths from the root Node, right? Well, in an ultrametric tree that would be true, however, not all trees are ultrametric. If tips are different distances from the root, then the height of a Node may need to be calculated relative to some other connected Node that is farthest from the root. This requires calculating the distance of every tip from the root, and then asking how far above some other tip your focal tip is located. Not too complicated, but the ability to do this quickly can speed up many operations. Remember from the [Coding Bootcamp] that we can measure the speed of executed code easily using the IPython magic command %timeit tree = toytree . rtree . unittree ( ntips = 10 , seed = 123 ) def get_ntips_example1 ( tree : toytree . ToyTree ) -> int : \"\"\"Return leaf Nodes by traversing tree.\"\"\" return sum ( 1 if i . is_leaf () for i in tree . traverse ()) def get_ntips_example2 ( tree : toytree . ToyTree ) -> int : \"\"\"Return leaf Nodes from cache.\"\"\" return tree . ntips %% timeit","title":"Python loves cached data"},{"location":"chapter-1/1.5-code-tree-class/#private-attributes","text":"These are termed private or hidden because they are not intended to be accessed by a user. The single underscore at the beginning of their name indicates this; stay away. It is further exemplified within interactive environments, like a jupyter notebook, where using tab-completion to view attributes and functions associated with an object will not show private attributes and functions by default, unless you begin to type the underscore. Why create private attributes and functions? There are actually several good reasons: (1) to only show users the code that they are intended to use; (2) to limit or specify how a user can modify an object; and (3) to limit to use. This can require a lot of careful work by the developers, and is often overlooked, but has a huge impact on making a package easy to learn and use.","title":"Private attributes"},{"location":"chapter-1/1.5-code-tree-class/#toytreenode-first-look","text":"Let's start by creating an example Node object that uses the @property decorator, and which only defines private attribute names in the __init__ . Remember that all attributes of a class should be defined in the __init__ function. So how will a user access data for this Node is all of its attributes are private? The answer is to use properties, which is easiest to do using the @property decorator. The user will now be able to see NodeExample1.name , which looks like an attribute, but is actually a property. It will return the _name attribute when called, and will call the associated setter function if the user tries to assign a new value to the property. This latter usage is perhaps the most important. It provides an opportunity to intercept the value that the user wants to assign to the property and to check it for various requirements before assigning it to the associated private attribute. For example, if a user tried to call node.name = 3 the setter function here intercepts the input value and converts it to a string by using the str() function before assigning it to the node._name attribute. class Node1 : \"\"\"A simplified version of toytree.Node used here for demonstration.\"\"\" def __init__ ( self , name : str = \"\" ): self . _name = name # non-init attributes for storing connections among Nodes self . up : Node = None self . children : Tuple [ 'Node1' ] = () @property def name ( self ) -> str : return self . _name @name . setter def name ( self , value : str ) -> None : self . _name = str ( value ) So we now have an improved method for setting the name of a Node, which will ensure that the value is always a string. What else can we use properties for? In toytree we use this framework to purposefully make Node objects immutable, meaning that do not want users to edit the attributes of Node's directly, for example by editing which Node is the parent of another. An example of this would be the following: node1 = Node1 ( '1' ) node2 = Node1 ( '2' ) node3 = Node1 ( '3' ) node1 . children = [ node2 , node3 ] The above operation is allowed using the simple Node class that we developed in the previous lesson. However, this is not a good operation, since the references among the objects is not complete. node1 has a reference to its two child Nodes, however, the child nodes do did not set node1 as their parent ( ._up attribute). One way to restrict users from making this error is to only allow them to modify .children or .up using functions that are designed to ensure that they are set appropriately. class Node2 : \"\"\"A simplified version of toytree.Node used here for demonstration.\"\"\" def __init__ ( self , name : str = \"\" ): self . _name = name # non-init attributes for storing connections among Nodes self . _children : Tuple [ Node ] = () self . _up : Optional [ Node ] = None @property def name ( self ) -> str : return self . _name @name . setter def name ( self , value : str ) -> None : self . _name = str ( value ) @property def children ( self ) -> Tuple [ Node ]: return self . _children @children . setter def children ( self , value : Tuple [ Node ]) -> None : raise TypeError ( \"'children' attribute does not allow item assignment\" ) @property def up ( self ) -> Node : return self . _up @up . setter def up ( self , value : Tuple [ Node ]) -> None : raise TypeError ( \"'up' attribute does not allow item assignment\" ) def add_child ( self , node : Node ) -> None : \"\"\"Add a Node as a connected child to this Node.\"\"\" node . _up = self self . _children += ( node , ) Now to assign a child the user must use the add_child function, which ensures that when a child is added the connections are made from both parent to child, and child to parent. If the user has an advanced understanding of the Node2 class they can of course work around this solution by modifying the hidden ._up and _children attributes, but in that case they know that they are editing attributes that were purposefully not exposed to the user, and so should not be surprised if they potentially encounter unexpected outcomes.","title":"toytree.Node first look"},{"location":"chapter-1/1.5-code-tree-class/#private-functions","text":"We may similarly wish to develop private functions. This is useful for functions that are for internal use by the developer, but are unlikely to be useful to users. By hiding them from the user it will present a more user-friendly object that directs them to use it in the best way possible.","title":"Private functions"},{"location":"chapter-1/1.5-code-tree-class/#an-immutable-node-class","text":"Have a look at the toytree.Node class object from the current main branch of toytree on GitHub. You can see that it follows the principle we can developed above but with a few additional private attributes. Most importantly, this includes the .height attribute of a Node.","title":"An immutable Node class"},{"location":"chapter-1/1.5-code-tree-class/#unique-node-labels-idx","text":"A set of connected Nodes can only be accessed by their traversal currently, or by data assigned to nodes, such as name attributes. We have learned how to use properties to limit how users assign values to a single object, but what if we want to assign data to a Node that takes into account the data of all other connected Nodes? An example would be assigning a unique number to every Node to serve as a label. ...","title":"Unique node labels (idx)"},{"location":"chapter-1/1.5-code-tree-class/#the-toytree-class-object","text":"As we've discussed previously, a tree-like structure can be represented simply by the connections among Node objects, so what use is there in creating a separate Tree class object? In toytree this was design choice was made because we decided there was a distinction in the intended way that a Tree versus Node object is intended to be used. A Node stores data including information about its direct edges, whereas a Tree stores information about a set of connected Nodes, and has functions meant for operating on them as a collection. With this in mind, we created a class object called ToyTree that is initialized with a Node object that represents the root Node of a tree (whether or not the tree is rooted). # design expectation tree = ToyTree ( root_node ) # access information about all Nodes connected to root_node print ( tree . nnodes ) print ( tree . ntips ) print ( tree . get_node_data ( \"height\" )) # traverse Nodes in desired order # edit Node attributes (returns a new tree with modified Node dists) tree = tree . set_node_data ( \"height\" , mapping = { 0 : 100 , 1 : 200 })","title":"The ToyTree class object"},{"location":"chapter-2/2.0-trees-as-models/","text":"Trees as models \u00b6 Learning objectives \u00b6 This chapter \u00b6","title":"Trees as models"},{"location":"chapter-2/2.0-trees-as-models/#trees-as-models","text":"","title":"Trees as models"},{"location":"chapter-2/2.0-trees-as-models/#learning-objectives","text":"This chapter","title":"Learning objectives"},{"location":"chapter-2/2.0-trees-as-models/#_1","text":"","title":""},{"location":"cookbooks/1-amaranthus-dioecy-api/","text":"Workflow diagram \u00b6 %%{init: {'theme': 'dark', \"flowchart\" : { \"curve\" : \"basis\" } } }%% graph LR A(kcount) B(kfilter) C(kextract) D(kassemble) A --> B --> C --> D Study description \u00b6 Here we implement the study by Neves et al. to detect a male linked genomic region involved in sex determination in the dioecious plant species Amaranthus palmeri . This study uses pool-seq to sequence four populations composing male and female plants from two geographically distinct populations. Ca\u0301tia Jose\u0301 Neves, Maor Matzrafi, Meik Thiele, Anne Lorant, Mohsen B Mesgaran, Markus G Stetter, Male Linked Genomic Region Determines Sex in Dioecious Amaranthus palmeri, Journal of Heredity, Volume 111, Issue 7, October 2020, Pages 606\u2013612, https://doi.org/10.1093/jhered/esaa047 Get the fastq data \u00b6 If you wish to follow along you can dowload the data with these instructions. download fastq data using wget # make a directory to store the raw fastq data files mkdir -p ./fastq-data URLS =( ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR416/001/ERR4161581/ERR4161581_1.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR416/001/ERR4161582/ERR4161582_1.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR416/001/ERR4161583/ERR4161583_1.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR416/001/ERR4161584/ERR4161584_1.fastq.gz ) # download files to the specified fastq directory for url in ERR4161581 ERR4161582 ERR4161583 ERR4161584 ; do wget $url ; done or, download fastq data using sra-tools Download the latest version of the sratools from https://github.com/ncbi/sra-tools/wiki/01.-Downloading-SRA-Toolkit by selecting the compiled binaries that are appropriate for your system (e.g., Linux or MacOSX). (I really do recommend that you use the latest version since this software is updated frequently and does not maintain compatibility with older versions. Follow the instructions to setup gcloud or aws to dramatically improve speed.) Then run the command below to download the fastq data for this study into a new directory. The total filesize will be about 140Gb. # make a directory to store the raw fastq data files mkdir -p ./fastq-data # download files to the specified fastq directory for run in ERR4161581 ERR4161582 ERR4161583 ERR4161584 ; do fasterq-dump --progress --outdir ./fastq-data --temp /tmp $run ; done Setup: imports and data files \u00b6 Set the logging level on kmerkit to specify more or less logged info. import kmerkit kmerkit . set_loglevel ( \"INFO\" ) Load the files to a dictionary mapping sample names to a list of file names. # create a dictionary mapping study sample names to list of file paths FASTQ_DICT = { \"california-male\" : [ \"ERS4574576_R1.fastq.gz\" , \"ERS4574576_R2.fastq.gz\" ], \"california-female\" : [ \"ERS4574577_R1.fastq.gz\" , \"ERS4574577_R2.fastq.gz\" ], \"kansas-male\" : [ \"ERS4574578_R1.fastq.gz\" , \"ERS4574578_R2.fastq.gz\" ], \"kansas-female\" : [ \"ERS4574579_R1.fastq.gz\" , \"ERS4574579_R2.fastq.gz\" ], } FASTQ_DICT grouping of sample names to list of file paths. Create kmer databases \u00b6 # init counter class object tool = Kcount ( fastq_dict = FASTQ_DICT , name = \"hybridus\" , workdir = \"/tmp/\" , kmersize = 35 , trim_reads = True , mindepth = 15 , maxdepth = 2000 , maxcount = 65535 , # max count possible with 16 bit integers canonical = True , ) # run the analysis tool . run () # optionally access a stats summary for the run print ( tool . statsdf ) kmerkit logged output ... Filter kmers: unique to males \u00b6 tool = Kfilter ( name = \"hybridus\" , workdir = \"/tmp\" , trait_dict = { 0 : [ \"california-female\" , \"kansas-female\" ], 1 : [ \"california-male\" , \"kansas-male\" ], }, mincov = 0.0 , # no global min applied. minmap = { 0 : 0.0 , # group 0 can have as little as 0 coverage 1 : 0.5 , # group 1 must have at least 50% coverage }, maxmap = { 0 : 0.0 , # group 0 must have 0 coverage 1 : 1.0 , # group 1 can have up to 100% coverage }, mincov_canon = { 0 : 0.0 , # no canonical min in group 0 1 : 0.5 , # kmer must be canonical in 50% of group 1 samples } ) tool . run () print ( tool . statsdf ) kmerkit logged output ... The resulting files are KMC database files written with the name prefix {workdir}/{name}-{sample-name}-kfilter.[pre,suf] peek at workdir file structure . |---workdir | ------- a |-------b --------c ... Extract reads unique to males \u00b6 # set up filter tool tool = Kextract ( name = \"hybridus\" , workdir = \"/tmp\" , fastq_path = FASTQ_DICT , group_kmers = \"/tmp/kfilter_hybridus_filtered\" , mindepth = 1 , ) tool . run () print ( tool . statsdf . T ) kmerkit logged output ... Assemble contigs from extracted reads \u00b6 For every --sample provided to kassemble kmerkit kextract \\ --name amaranth-dioecy \\ --workdir ./cookbook1 \\ --sample A ./data/sample-A.fastq.gz \\ --sample B ./data/sample-B.fastq.gz \\ --sample C ./data/sample-C.fastq.gz \\ kmerkit logged output ...","title":"1 amaranthus dioecy api"},{"location":"cookbooks/1-amaranthus-dioecy-api/#workflow-diagram","text":"%%{init: {'theme': 'dark', \"flowchart\" : { \"curve\" : \"basis\" } } }%% graph LR A(kcount) B(kfilter) C(kextract) D(kassemble) A --> B --> C --> D","title":"Workflow diagram"},{"location":"cookbooks/1-amaranthus-dioecy-api/#study-description","text":"Here we implement the study by Neves et al. to detect a male linked genomic region involved in sex determination in the dioecious plant species Amaranthus palmeri . This study uses pool-seq to sequence four populations composing male and female plants from two geographically distinct populations. Ca\u0301tia Jose\u0301 Neves, Maor Matzrafi, Meik Thiele, Anne Lorant, Mohsen B Mesgaran, Markus G Stetter, Male Linked Genomic Region Determines Sex in Dioecious Amaranthus palmeri, Journal of Heredity, Volume 111, Issue 7, October 2020, Pages 606\u2013612, https://doi.org/10.1093/jhered/esaa047","title":"Study description"},{"location":"cookbooks/1-amaranthus-dioecy-api/#get-the-fastq-data","text":"If you wish to follow along you can dowload the data with these instructions. download fastq data using wget # make a directory to store the raw fastq data files mkdir -p ./fastq-data URLS =( ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR416/001/ERR4161581/ERR4161581_1.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR416/001/ERR4161582/ERR4161582_1.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR416/001/ERR4161583/ERR4161583_1.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR416/001/ERR4161584/ERR4161584_1.fastq.gz ) # download files to the specified fastq directory for url in ERR4161581 ERR4161582 ERR4161583 ERR4161584 ; do wget $url ; done or, download fastq data using sra-tools Download the latest version of the sratools from https://github.com/ncbi/sra-tools/wiki/01.-Downloading-SRA-Toolkit by selecting the compiled binaries that are appropriate for your system (e.g., Linux or MacOSX). (I really do recommend that you use the latest version since this software is updated frequently and does not maintain compatibility with older versions. Follow the instructions to setup gcloud or aws to dramatically improve speed.) Then run the command below to download the fastq data for this study into a new directory. The total filesize will be about 140Gb. # make a directory to store the raw fastq data files mkdir -p ./fastq-data # download files to the specified fastq directory for run in ERR4161581 ERR4161582 ERR4161583 ERR4161584 ; do fasterq-dump --progress --outdir ./fastq-data --temp /tmp $run ; done","title":"Get the fastq data"},{"location":"cookbooks/1-amaranthus-dioecy-api/#setup-imports-and-data-files","text":"Set the logging level on kmerkit to specify more or less logged info. import kmerkit kmerkit . set_loglevel ( \"INFO\" ) Load the files to a dictionary mapping sample names to a list of file names. # create a dictionary mapping study sample names to list of file paths FASTQ_DICT = { \"california-male\" : [ \"ERS4574576_R1.fastq.gz\" , \"ERS4574576_R2.fastq.gz\" ], \"california-female\" : [ \"ERS4574577_R1.fastq.gz\" , \"ERS4574577_R2.fastq.gz\" ], \"kansas-male\" : [ \"ERS4574578_R1.fastq.gz\" , \"ERS4574578_R2.fastq.gz\" ], \"kansas-female\" : [ \"ERS4574579_R1.fastq.gz\" , \"ERS4574579_R2.fastq.gz\" ], } FASTQ_DICT grouping of sample names to list of file paths.","title":"Setup: imports and data files"},{"location":"cookbooks/1-amaranthus-dioecy-api/#create-kmer-databases","text":"# init counter class object tool = Kcount ( fastq_dict = FASTQ_DICT , name = \"hybridus\" , workdir = \"/tmp/\" , kmersize = 35 , trim_reads = True , mindepth = 15 , maxdepth = 2000 , maxcount = 65535 , # max count possible with 16 bit integers canonical = True , ) # run the analysis tool . run () # optionally access a stats summary for the run print ( tool . statsdf ) kmerkit logged output ...","title":"Create kmer databases"},{"location":"cookbooks/1-amaranthus-dioecy-api/#filter-kmers-unique-to-males","text":"tool = Kfilter ( name = \"hybridus\" , workdir = \"/tmp\" , trait_dict = { 0 : [ \"california-female\" , \"kansas-female\" ], 1 : [ \"california-male\" , \"kansas-male\" ], }, mincov = 0.0 , # no global min applied. minmap = { 0 : 0.0 , # group 0 can have as little as 0 coverage 1 : 0.5 , # group 1 must have at least 50% coverage }, maxmap = { 0 : 0.0 , # group 0 must have 0 coverage 1 : 1.0 , # group 1 can have up to 100% coverage }, mincov_canon = { 0 : 0.0 , # no canonical min in group 0 1 : 0.5 , # kmer must be canonical in 50% of group 1 samples } ) tool . run () print ( tool . statsdf ) kmerkit logged output ... The resulting files are KMC database files written with the name prefix {workdir}/{name}-{sample-name}-kfilter.[pre,suf] peek at workdir file structure . |---workdir | ------- a |-------b --------c ...","title":"Filter kmers: unique to males"},{"location":"cookbooks/1-amaranthus-dioecy-api/#extract-reads-unique-to-males","text":"# set up filter tool tool = Kextract ( name = \"hybridus\" , workdir = \"/tmp\" , fastq_path = FASTQ_DICT , group_kmers = \"/tmp/kfilter_hybridus_filtered\" , mindepth = 1 , ) tool . run () print ( tool . statsdf . T ) kmerkit logged output ...","title":"Extract reads unique to males"},{"location":"cookbooks/1-amaranthus-dioecy-api/#assemble-contigs-from-extracted-reads","text":"For every --sample provided to kassemble kmerkit kextract \\ --name amaranth-dioecy \\ --workdir ./cookbook1 \\ --sample A ./data/sample-A.fastq.gz \\ --sample B ./data/sample-B.fastq.gz \\ --sample C ./data/sample-C.fastq.gz \\ kmerkit logged output ...","title":"Assemble contigs from extracted reads"},{"location":"cookbooks/1-amaranthus-dioecy/","text":"Workflow diagram \u00b6 %%{init: {'theme': 'dark', \"flowchart\" : { \"curve\" : \"basis\" } } }%% graph LR 0(kinit) 1(ktrim) A(kcount) B(kfilter) C(kextract) D(kassemble) 0 --> 1 --> A --> B --> C --> D linkStyle default stroke-width:2px,fill:none,stroke:grey; Study description \u00b6 Here we re-implement the study by Neves et al. to detect a male linked genomic region involved in sex determination in the dioecious plant species Amaranthus palmeri . This study uses pool-seq to sequence four populations composing male and female plants from two geographically distinct populations. Ca\u0301tia Jose\u0301 Neves, Maor Matzrafi, Meik Thiele, Anne Lorant, Mohsen B Mesgaran, Markus G Stetter, Male Linked Genomic Region Determines Sex in Dioecious Amaranthus palmeri, Journal of Heredity, Volume 111, Issue 7, October 2020, Pages 606\u2013612, https://doi.org/10.1093/jhered/esaa047 Fetch fastq data \u00b6 If you wish to follow along you can dowload the data from ERA with the instructions below: ERR4161581 ( California_male_pool ), ERR4161582 ( California_female_pool ), ERR4161583 ( Kansas_male_pool ), ERR4161584 ( Kansas_female_pool ) Download fastq data using sra-tools Download the latest version of the sratools from https://github.com/ncbi/sra-tools/wiki/01.-Downloading-SRA-Toolkit by selecting the compiled binaries that are appropriate for your system (e.g., Linux or MacOSX). (I really do recommend that you use the latest version since this software is updated frequently and does not maintain compatibility with older versions.) Run the command below to download the fastq data for this study into a new directory. The total file size will be about 140Gb. # make a directory to store the raw fastq data files mkdir -p ./fastq-data # download files to the specified fastq directory for run in ERR4161581 ERR4161582 ERR4161583 ERR4161584 ; do fasterq-dump --progress --outdir ./fastq-data --temp . $run ; done Kmerkit analysis \u00b6 Initialize a new project \u00b6 Start a new project by entering a name and working directory to init , representing the filename prefix and location where all files will be saved. The directory will be created if it doesn't yet exist. Multiple input fastq files can be entered as arguments, or multiple can be selected using a regular expression selector like below. Paired reads are automatically detected based on name matching. Sample names are automatically extracted from file names. Paired file names should be identical up until some delimiter (e.g., _ ), after which the tail is trimmed ( _R1.fastq.gz ). You can set the delimiter explicitly. kmerkit init --name dioecy --workdir /tmp --delim \"_\" ./fastq-data/*.gz This step creates a project JSON file, which will contain fully reproducible information about each step in a kmerkit analysis. It will be updated upon each kmerkit module that is run. This file can be read directly, or, you can view it easily by calling kmerkit stats , like below. kmerkit stats output kmerkit stats --json /tmp/dioecy.json Projec t JSON da ta : { \"name\" : \"Neves\" , \"workdir\" : \"/pinky/deren/palmeri\" , \"versions\" : { \"kmerkit\" : \"0.0.12\" , \"kmc\" : \"3.1.1\" , \"gemma\" : \"0.9.83\" , \"sklearn\" : \"0.24.1\" }, \"kinit\" : { \"data\" : { \"ERR4161581\" : [ \"/pinky/DATASTORE/Amaranthus_palmeri_PRJEB38372/ERR4161581_1.fastq.gz\" , \"/pinky/DATASTORE/Amaranthus_palmeri_PRJEB38372/ERR4161581_2.fastq.gz\" ], \"ERR4161582\" : [ \"/pinky/DATASTORE/Amaranthus_palmeri_PRJEB38372/ERR4161582_1.fastq.gz\" , \"/pinky/DATASTORE/Amaranthus_palmeri_PRJEB38372/ERR4161582_2.fastq.gz\" ], \"ERR4161583\" : [ \"/pinky/DATASTORE/Amaranthus_palmeri_PRJEB38372/ERR4161583_1.fastq.gz\" , \"/pinky/DATASTORE/Amaranthus_palmeri_PRJEB38372/ERR4161583_2.fastq.gz\" ], \"ERR4161584\" : [ \"/pinky/DATASTORE/Amaranthus_palmeri_PRJEB38372/ERR4161584_1.fastq.gz\" , \"/pinky/DATASTORE/Amaranthus_palmeri_PRJEB38372/ERR4161584_2.fastq.gz\" ] }, \"commands\" : {} }, } Read trimming (optional) \u00b6 You can perform read trimming on your own before using kmerkit, but we also provide the option to trim, filter, or subsample your reads within kmerkit by calling the program fastp . Trimmed read files are written to the workdir, and subsequent modules (e.g., count and extract ) will use the trimmed reads instead of the raw reads (both sets of filepaths can be viewed in the project JSON file). kmerkit trim -j /tmp/dioecy.json --workers 4 Like before we can access the results from the JSON file by calling the stats module. In this case I also provide a module name (trim) to select only results for this module. kmerkit stats output kmerkit stats -j /tmp/dioecy.json trim ... Count kmers \u00b6 Kmers are counted for each sample using kmc at the specified kmer-size. Kmers occurring above or below the specified thresholds will be excluded. See the kmerkit count page for details on parallelization and memory consumption for optimizing the speed of this step, which is usually the most time consuming. kmerkit count -j /tmp/dioecy.json --kmer-size 35 --min-depth 5 --workers 4 kmerkit stats output kmerkit stats -j /tmp/dioecy.json count ... Filter kmers \u00b6 Apply filters to identify target kmers that are enriched in one group of samples versus another. In this case, we aim to identify male-specific kmers, meaning those that are present in males but not females. This can be done by setting a high min-map for group 1 (kmers must be present in group 1) and setting a low max-map for group 0 (kmers cannot be present in group 0). We must also assign samples to groups 0 or 1. For studies with many samples this is most easily done by entering a CSV file (see the kfilter docs section). Here because there are few samples I use the simpler option of entering the sample names directly using the -0 and -1 options to assign to their respective groups. The min_map and max_map entries each take two ordered values, assigned to group 0 and 1, respectively. kmerkit filter \\ --json /tmp/dioecy.json \\ -1 'ERR4161581' -1 'ERR4161583' \\ -0 'ERR4161582' -0 'ERR4161584' \\ --min_map 0 .0 0 .5 \\ --max_map 0 .0 1 .0 kmerkit filter results kmerkit stats -j /tmp/dioecy.json filter ... Extract reads \u00b6 Now that we've identified a set of target kmers we will extract reads from fastq data files that contain these kmers. This is expected to pull out reads mapping to male-specific regions of the A. palmeri genome. Here you can enter new fastq files to extract data from, or enter the names of samples already in the project database, which will use the (trimmed) fastq data files referenced in the JSON file. Here I don't enter any sample names, which defaults to performing extractions on all 4 samples in the database (we expect to not recover any reads for the two female pop samples). kmerkit extract --json /tmp/dioecy.json --min-kmers-per-read 5 kmerkit stats output ... Assemble contigs \u00b6 From the extracted reads created in the last step, we can now assemble contigs for each sample (or for all samples pooled together) using the assemble module. Here we use the default assembler, spades. This creates a number of output files in the workdir, which can be summarized with stats . kmerkit assemble --json /tmp/dioecy.json ... The main result of Neves et al. was the identification of an approximately 2Mb assembled contig representing a large contiguous male-specific genomic region. The summary here shows the ... kmerkit stats output ... Reproducibility \u00b6 In addition to your scripts that can be used to reproduce your analysis, the JSON project file contains a full record of the samples, parameters, and the order of analysis steps that make up your analysis. kmerkit project JSON file ... TODO: Post-pipeline analysis API \u00b6 The kmerkit Python API can be used to perform post-pipeline analyses in a jupyter notebook. Here we create a plot of import kmerkit project = kmerkit . load_json ( \"/tmp/dioecy.json\" ) ...","title":"1 amaranthus dioecy"},{"location":"cookbooks/1-amaranthus-dioecy/#workflow-diagram","text":"%%{init: {'theme': 'dark', \"flowchart\" : { \"curve\" : \"basis\" } } }%% graph LR 0(kinit) 1(ktrim) A(kcount) B(kfilter) C(kextract) D(kassemble) 0 --> 1 --> A --> B --> C --> D linkStyle default stroke-width:2px,fill:none,stroke:grey;","title":"Workflow diagram"},{"location":"cookbooks/1-amaranthus-dioecy/#study-description","text":"Here we re-implement the study by Neves et al. to detect a male linked genomic region involved in sex determination in the dioecious plant species Amaranthus palmeri . This study uses pool-seq to sequence four populations composing male and female plants from two geographically distinct populations. Ca\u0301tia Jose\u0301 Neves, Maor Matzrafi, Meik Thiele, Anne Lorant, Mohsen B Mesgaran, Markus G Stetter, Male Linked Genomic Region Determines Sex in Dioecious Amaranthus palmeri, Journal of Heredity, Volume 111, Issue 7, October 2020, Pages 606\u2013612, https://doi.org/10.1093/jhered/esaa047","title":"Study description"},{"location":"cookbooks/1-amaranthus-dioecy/#fetch-fastq-data","text":"If you wish to follow along you can dowload the data from ERA with the instructions below: ERR4161581 ( California_male_pool ), ERR4161582 ( California_female_pool ), ERR4161583 ( Kansas_male_pool ), ERR4161584 ( Kansas_female_pool ) Download fastq data using sra-tools Download the latest version of the sratools from https://github.com/ncbi/sra-tools/wiki/01.-Downloading-SRA-Toolkit by selecting the compiled binaries that are appropriate for your system (e.g., Linux or MacOSX). (I really do recommend that you use the latest version since this software is updated frequently and does not maintain compatibility with older versions.) Run the command below to download the fastq data for this study into a new directory. The total file size will be about 140Gb. # make a directory to store the raw fastq data files mkdir -p ./fastq-data # download files to the specified fastq directory for run in ERR4161581 ERR4161582 ERR4161583 ERR4161584 ; do fasterq-dump --progress --outdir ./fastq-data --temp . $run ; done","title":"Fetch fastq data"},{"location":"cookbooks/1-amaranthus-dioecy/#kmerkit-analysis","text":"","title":"Kmerkit analysis"},{"location":"cookbooks/1-amaranthus-dioecy/#initialize-a-new-project","text":"Start a new project by entering a name and working directory to init , representing the filename prefix and location where all files will be saved. The directory will be created if it doesn't yet exist. Multiple input fastq files can be entered as arguments, or multiple can be selected using a regular expression selector like below. Paired reads are automatically detected based on name matching. Sample names are automatically extracted from file names. Paired file names should be identical up until some delimiter (e.g., _ ), after which the tail is trimmed ( _R1.fastq.gz ). You can set the delimiter explicitly. kmerkit init --name dioecy --workdir /tmp --delim \"_\" ./fastq-data/*.gz This step creates a project JSON file, which will contain fully reproducible information about each step in a kmerkit analysis. It will be updated upon each kmerkit module that is run. This file can be read directly, or, you can view it easily by calling kmerkit stats , like below. kmerkit stats output kmerkit stats --json /tmp/dioecy.json Projec t JSON da ta : { \"name\" : \"Neves\" , \"workdir\" : \"/pinky/deren/palmeri\" , \"versions\" : { \"kmerkit\" : \"0.0.12\" , \"kmc\" : \"3.1.1\" , \"gemma\" : \"0.9.83\" , \"sklearn\" : \"0.24.1\" }, \"kinit\" : { \"data\" : { \"ERR4161581\" : [ \"/pinky/DATASTORE/Amaranthus_palmeri_PRJEB38372/ERR4161581_1.fastq.gz\" , \"/pinky/DATASTORE/Amaranthus_palmeri_PRJEB38372/ERR4161581_2.fastq.gz\" ], \"ERR4161582\" : [ \"/pinky/DATASTORE/Amaranthus_palmeri_PRJEB38372/ERR4161582_1.fastq.gz\" , \"/pinky/DATASTORE/Amaranthus_palmeri_PRJEB38372/ERR4161582_2.fastq.gz\" ], \"ERR4161583\" : [ \"/pinky/DATASTORE/Amaranthus_palmeri_PRJEB38372/ERR4161583_1.fastq.gz\" , \"/pinky/DATASTORE/Amaranthus_palmeri_PRJEB38372/ERR4161583_2.fastq.gz\" ], \"ERR4161584\" : [ \"/pinky/DATASTORE/Amaranthus_palmeri_PRJEB38372/ERR4161584_1.fastq.gz\" , \"/pinky/DATASTORE/Amaranthus_palmeri_PRJEB38372/ERR4161584_2.fastq.gz\" ] }, \"commands\" : {} }, }","title":"Initialize a new project"},{"location":"cookbooks/1-amaranthus-dioecy/#read-trimming-optional","text":"You can perform read trimming on your own before using kmerkit, but we also provide the option to trim, filter, or subsample your reads within kmerkit by calling the program fastp . Trimmed read files are written to the workdir, and subsequent modules (e.g., count and extract ) will use the trimmed reads instead of the raw reads (both sets of filepaths can be viewed in the project JSON file). kmerkit trim -j /tmp/dioecy.json --workers 4 Like before we can access the results from the JSON file by calling the stats module. In this case I also provide a module name (trim) to select only results for this module. kmerkit stats output kmerkit stats -j /tmp/dioecy.json trim ...","title":"Read trimming (optional)"},{"location":"cookbooks/1-amaranthus-dioecy/#count-kmers","text":"Kmers are counted for each sample using kmc at the specified kmer-size. Kmers occurring above or below the specified thresholds will be excluded. See the kmerkit count page for details on parallelization and memory consumption for optimizing the speed of this step, which is usually the most time consuming. kmerkit count -j /tmp/dioecy.json --kmer-size 35 --min-depth 5 --workers 4 kmerkit stats output kmerkit stats -j /tmp/dioecy.json count ...","title":"Count kmers"},{"location":"cookbooks/1-amaranthus-dioecy/#filter-kmers","text":"Apply filters to identify target kmers that are enriched in one group of samples versus another. In this case, we aim to identify male-specific kmers, meaning those that are present in males but not females. This can be done by setting a high min-map for group 1 (kmers must be present in group 1) and setting a low max-map for group 0 (kmers cannot be present in group 0). We must also assign samples to groups 0 or 1. For studies with many samples this is most easily done by entering a CSV file (see the kfilter docs section). Here because there are few samples I use the simpler option of entering the sample names directly using the -0 and -1 options to assign to their respective groups. The min_map and max_map entries each take two ordered values, assigned to group 0 and 1, respectively. kmerkit filter \\ --json /tmp/dioecy.json \\ -1 'ERR4161581' -1 'ERR4161583' \\ -0 'ERR4161582' -0 'ERR4161584' \\ --min_map 0 .0 0 .5 \\ --max_map 0 .0 1 .0 kmerkit filter results kmerkit stats -j /tmp/dioecy.json filter ...","title":"Filter kmers"},{"location":"cookbooks/1-amaranthus-dioecy/#extract-reads","text":"Now that we've identified a set of target kmers we will extract reads from fastq data files that contain these kmers. This is expected to pull out reads mapping to male-specific regions of the A. palmeri genome. Here you can enter new fastq files to extract data from, or enter the names of samples already in the project database, which will use the (trimmed) fastq data files referenced in the JSON file. Here I don't enter any sample names, which defaults to performing extractions on all 4 samples in the database (we expect to not recover any reads for the two female pop samples). kmerkit extract --json /tmp/dioecy.json --min-kmers-per-read 5 kmerkit stats output ...","title":"Extract reads"},{"location":"cookbooks/1-amaranthus-dioecy/#assemble-contigs","text":"From the extracted reads created in the last step, we can now assemble contigs for each sample (or for all samples pooled together) using the assemble module. Here we use the default assembler, spades. This creates a number of output files in the workdir, which can be summarized with stats . kmerkit assemble --json /tmp/dioecy.json ... The main result of Neves et al. was the identification of an approximately 2Mb assembled contig representing a large contiguous male-specific genomic region. The summary here shows the ... kmerkit stats output ...","title":"Assemble contigs"},{"location":"cookbooks/1-amaranthus-dioecy/#reproducibility","text":"In addition to your scripts that can be used to reproduce your analysis, the JSON project file contains a full record of the samples, parameters, and the order of analysis steps that make up your analysis. kmerkit project JSON file ...","title":"Reproducibility"},{"location":"cookbooks/1-amaranthus-dioecy/#todo-post-pipeline-analysis-api","text":"The kmerkit Python API can be used to perform post-pipeline analyses in a jupyter notebook. Here we create a plot of import kmerkit project = kmerkit . load_json ( \"/tmp/dioecy.json\" ) ...","title":"TODO: Post-pipeline analysis API"},{"location":"cookbooks/2-arabidopsis-gwas/","text":"Workflow diagram \u00b6 %%{init: {'theme': 'base', 'themeVariables': { 'primaryColor': 'grey', 'titleColor': '#262626', 'background': green}}}%% graph LR A(kcount) B(kfilter) C(kmatrix) D(kgwas) subgraph kmerkit modules A --> B --> C --> D end style A fill:teal,stroke:#333,stroke-width:2px,color:#262626 style B fill:goldenrod,stroke:#333,stroke-width:2px,color:#262626 Study description \u00b6 Explain and cite Arabidopsis studies here... ... \u00b6","title":"2 arabidopsis gwas"},{"location":"cookbooks/2-arabidopsis-gwas/#workflow-diagram","text":"%%{init: {'theme': 'base', 'themeVariables': { 'primaryColor': 'grey', 'titleColor': '#262626', 'background': green}}}%% graph LR A(kcount) B(kfilter) C(kmatrix) D(kgwas) subgraph kmerkit modules A --> B --> C --> D end style A fill:teal,stroke:#333,stroke-width:2px,color:#262626 style B fill:goldenrod,stroke:#333,stroke-width:2px,color:#262626","title":"Workflow diagram"},{"location":"cookbooks/2-arabidopsis-gwas/#study-description","text":"Explain and cite Arabidopsis studies here...","title":"Study description"},{"location":"cookbooks/2-arabidopsis-gwas/#_1","text":"","title":"..."},{"location":"cookbooks/3-pedicularis-RAD-gwas/","text":"Workflow diagram \u00b6 %%{init: {'theme': 'dark', \"flowchart\" : { \"curve\" : \"basis\" } } }%% graph LR 0(kinit) 1(ktrim) A(kcount) B(kfilter) C(kmatrix) D(kgwas) 0 --> 1 --> A --> B --> C --> D linkStyle default stroke-width:2px,fill:none,stroke:grey; Study description \u00b6 Here we re-implement the study by Eaton et al. (in prep) to detect genomic loci associated with a novel floral trait in the plant species complex Pedicularis cranolopha . This study uses single-end RAD-seq data to sequence 100 individuals from 18 populations which vary in presence/absence of the trait of interest in a way that is discordant with the phylogeny. Citation in future Fetch fastq data \u00b6 If you wish to follow along you can dowload the data from SRA with the instructions below. Download fastq data using sra-tools Download the latest version of the sratools from https://github.com/ncbi/sra-tools/wiki/01.-Downloading-SRA-Toolkit by selecting the compiled binaries that are appropriate for your system (e.g., Linux or MacOSX). (I really do recommend that you use the latest version since this software is updated frequently and does not maintain compatibility with older versions.) Run the command below to download the fastq data for this study into a new directory. # make a directory to store the raw fastq data files mkdir -p ./fastq-data # download files to the specified fastq directory for run in ERR4161581 ERR4161582 ERR4161583 ERR4161584 ; do fasterq-dump --progress --outdir ./fastq-data --temp . $run ; done Kmerkit analysis \u00b6 Initialize a new project \u00b6 We start by initing a new project to assign samples names to files. The data are single-end kmerkit init --name forked --workdir /tmp --delim \"_\" ./fastq-data/*.gz This step creates a project JSON file, which will contain the fully reproducible information about each step in a kmerkit analysis. This file is updated upon each kmerkit module that is run. This file can be read directly, or, you can access a more nicely formatted view of specific results by calling kmerkit stats and specifying a specific module. kmerkit stats --json /tmp/forked.json init kmerkit stats output ... Read trimming (optional) \u00b6 We applied read-trimming using the default options in fastp , but also used the option --subsample 2e6 to normalize the number of reads among samples to a maximum of 2M. kmerkit trim --json /tmp/forked.json --workers 4 kmerkit stats --json /tmp/forked.json trim kmerkit stats output ... Count kmers \u00b6 Because RAD-seq generates data that extends directionally away from a cut-site the resulting reads will have specific orientations. Thus, we turn off canonical kmer counting here to count each kmer and its reverse-complement as separate patterns. kmerkit count --json /tmp/forked.json \\ --no-canonical --kmer-size 17 --min-depth 5 --workers 4 kmerkit stats --json /tmp/forked.json count kmerkit stats output ... Filter kmers \u00b6 Here I assign samples to either the target group (1) or the filter group (0) and keep kmers that occur in the frequency range 0.5-1.0 in the target, and exclude any that occur at all (i.e., in the range 0.0-1.0) in the filter group. kmerkit filter \\ --json /tmp/forked.json \\ -1 '...' \\ -0 '...' \\ --min_map 0 .0 0 .5 \\ --max_map 0 .0 1 .0 kmerkit stats -j /tmp/dioecy.json filter kmerkit filter results ... Perform GWAS \u00b6 Now that we've identified a set of target kmers we will extract reads from fastq data files that contain these kmers. This is expected to pull out reads mapping to male-specific regions of the A. palmeri genome. Here you can enter new fastq files to extract data from, or enter the names of samples already in the project database, which will use the (trimmed) fastq data files referenced in the JSON file. Here I don't enter any sample names, which defaults to performing extractions on all 4 samples in the database (we expect to not recover any reads for the two female pop samples). kmerkit gwas --json /tmp/forked.json --kinship --logistic kmerkit stats output ... Reproducibility \u00b6 In addition to your scripts that can be used to reproduce your analysis, the JSON project file contains a full record of the samples, parameters, and the order of analysis steps that make up your analysis. kmerkit project JSON file ... TODO: Post-pipeline analysis/visualization API \u00b6 The kmerkit Python API can be used to perform post-pipeline analyses in a jupyter notebook. Here we create a plot of import kmerkit project = kmerkit . load_json ( \"/tmp/forked.json\" ) ...","title":"3 pedicularis RAD gwas"},{"location":"cookbooks/3-pedicularis-RAD-gwas/#workflow-diagram","text":"%%{init: {'theme': 'dark', \"flowchart\" : { \"curve\" : \"basis\" } } }%% graph LR 0(kinit) 1(ktrim) A(kcount) B(kfilter) C(kmatrix) D(kgwas) 0 --> 1 --> A --> B --> C --> D linkStyle default stroke-width:2px,fill:none,stroke:grey;","title":"Workflow diagram"},{"location":"cookbooks/3-pedicularis-RAD-gwas/#study-description","text":"Here we re-implement the study by Eaton et al. (in prep) to detect genomic loci associated with a novel floral trait in the plant species complex Pedicularis cranolopha . This study uses single-end RAD-seq data to sequence 100 individuals from 18 populations which vary in presence/absence of the trait of interest in a way that is discordant with the phylogeny. Citation in future","title":"Study description"},{"location":"cookbooks/3-pedicularis-RAD-gwas/#fetch-fastq-data","text":"If you wish to follow along you can dowload the data from SRA with the instructions below. Download fastq data using sra-tools Download the latest version of the sratools from https://github.com/ncbi/sra-tools/wiki/01.-Downloading-SRA-Toolkit by selecting the compiled binaries that are appropriate for your system (e.g., Linux or MacOSX). (I really do recommend that you use the latest version since this software is updated frequently and does not maintain compatibility with older versions.) Run the command below to download the fastq data for this study into a new directory. # make a directory to store the raw fastq data files mkdir -p ./fastq-data # download files to the specified fastq directory for run in ERR4161581 ERR4161582 ERR4161583 ERR4161584 ; do fasterq-dump --progress --outdir ./fastq-data --temp . $run ; done","title":"Fetch fastq data"},{"location":"cookbooks/3-pedicularis-RAD-gwas/#kmerkit-analysis","text":"","title":"Kmerkit analysis"},{"location":"cookbooks/3-pedicularis-RAD-gwas/#initialize-a-new-project","text":"We start by initing a new project to assign samples names to files. The data are single-end kmerkit init --name forked --workdir /tmp --delim \"_\" ./fastq-data/*.gz This step creates a project JSON file, which will contain the fully reproducible information about each step in a kmerkit analysis. This file is updated upon each kmerkit module that is run. This file can be read directly, or, you can access a more nicely formatted view of specific results by calling kmerkit stats and specifying a specific module. kmerkit stats --json /tmp/forked.json init kmerkit stats output ...","title":"Initialize a new project"},{"location":"cookbooks/3-pedicularis-RAD-gwas/#read-trimming-optional","text":"We applied read-trimming using the default options in fastp , but also used the option --subsample 2e6 to normalize the number of reads among samples to a maximum of 2M. kmerkit trim --json /tmp/forked.json --workers 4 kmerkit stats --json /tmp/forked.json trim kmerkit stats output ...","title":"Read trimming (optional)"},{"location":"cookbooks/3-pedicularis-RAD-gwas/#count-kmers","text":"Because RAD-seq generates data that extends directionally away from a cut-site the resulting reads will have specific orientations. Thus, we turn off canonical kmer counting here to count each kmer and its reverse-complement as separate patterns. kmerkit count --json /tmp/forked.json \\ --no-canonical --kmer-size 17 --min-depth 5 --workers 4 kmerkit stats --json /tmp/forked.json count kmerkit stats output ...","title":"Count kmers"},{"location":"cookbooks/3-pedicularis-RAD-gwas/#filter-kmers","text":"Here I assign samples to either the target group (1) or the filter group (0) and keep kmers that occur in the frequency range 0.5-1.0 in the target, and exclude any that occur at all (i.e., in the range 0.0-1.0) in the filter group. kmerkit filter \\ --json /tmp/forked.json \\ -1 '...' \\ -0 '...' \\ --min_map 0 .0 0 .5 \\ --max_map 0 .0 1 .0 kmerkit stats -j /tmp/dioecy.json filter kmerkit filter results ...","title":"Filter kmers"},{"location":"cookbooks/3-pedicularis-RAD-gwas/#perform-gwas","text":"Now that we've identified a set of target kmers we will extract reads from fastq data files that contain these kmers. This is expected to pull out reads mapping to male-specific regions of the A. palmeri genome. Here you can enter new fastq files to extract data from, or enter the names of samples already in the project database, which will use the (trimmed) fastq data files referenced in the JSON file. Here I don't enter any sample names, which defaults to performing extractions on all 4 samples in the database (we expect to not recover any reads for the two female pop samples). kmerkit gwas --json /tmp/forked.json --kinship --logistic kmerkit stats output ...","title":"Perform GWAS"},{"location":"cookbooks/3-pedicularis-RAD-gwas/#reproducibility","text":"In addition to your scripts that can be used to reproduce your analysis, the JSON project file contains a full record of the samples, parameters, and the order of analysis steps that make up your analysis. kmerkit project JSON file ...","title":"Reproducibility"},{"location":"cookbooks/3-pedicularis-RAD-gwas/#todo-post-pipeline-analysisvisualization-api","text":"The kmerkit Python API can be used to perform post-pipeline analyses in a jupyter notebook. Here we create a plot of import kmerkit project = kmerkit . load_json ( \"/tmp/forked.json\" ) ...","title":"TODO: Post-pipeline analysis/visualization API"}]}